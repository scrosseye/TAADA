# TAADA

This repository includes the source code for a decoding tool that builds on the work of Saha et al., (2021): the Tool for the Automatic Analysis of Decoding Ambiguity (TAADA). TAADA is a freely available natural language processing (NLP) tool specifically designed to annotate and count lexical and sub-lexical features related to decoding in English. The features include metrics for grapheme, phoneme, and syllable counts, word frequency, contextual diversity, neighborhood effects, rhymes, and conditional probability. These are described below

Basic decoding counts. TAADA includes a number of basic decoding counts derived from the CMU pronouncing dictionary. The CMUdict provides a mapping between a word and that word’s phonemes for 133,779 words in the English language. However, in practice, many of these words are extremely rare. 
Prior to use, the CMUdict was cleaned of features that would influence decoding counts. These including removing alternative pronunciations for words, the characters ‘and . from words, and any redundant words. After cleaning, the CMUdict included 121,753 words. For these remaining words, we wrote a Python script to calculate the number of letters per word, the number of phonemes per word, and the number of syllables per word (by counting the number of vowel phonemes in each word). We calculated the average length of syllables by dividing the number of letters in a word by the number of syllables. We calculated a raw discrepancy count by subtracting the number of phonemes by the number of letters for each word and a discrepancy ratio by dividing the number of phonemes by the number of letters for each word. We also calculated the number of consonant characters, vowel characters, consonant phonemes, and consonant vowels per word. Lastly, from these counts, we calculated the average phonemes per consonant characters, vowel characters, and all characters per word.

Word frequency and contextual diversity counts. From the ELP database, we extracted SUBTLEXus frequency counts and contextual diversity (i.e., the number of samples in which a word occurs) counts for each of the words in the CMUDict that overlapped (~50,000 words) with the ELP database. SUBTLEXus is a corpus of subtitles from 8,388 American films and television shows that comprise 51 million words (Brysbaert & New, 2009). We specifically selected word frequency and contextual diversity scores from SUBTLEXus that were logarithmically transformed (log10) to control for Zipfian distributions common to frequency measures (Zipf, 1946). We also derived frequency and contextual diversity scores from the Corpus of Contemporary English (COCA). COCA contains more than one billion words from texts produced from 1990-2019 and includes eight genres: subtitles, spoken, fiction, magazine, web pages, blogs, newspapers, and academic texts. We focused on frequency scores from the magazine genre because the samples were most representative of average language use. Like the SUBTLEXus scores, the COCA values were also logarithmically transformed. COCA frequency and contextual diversity scores were available for ~65,000 of the words in the CMUDict.

Neighborhood effect counts. TAADA includes word neighborhood measures reported in the English Lexicon Project (ELP) database. The ELP database includes counts for orthographic, phonologic, and phonographic neighbors and their frequency. Neighborhood counts are available for ~45,000 words in the ELP while neighborhood frequency counts are available for between ~22,000 and ~38,000 words depending on the measure. 

Orthographic neighbors. The ELP database contains measures related to orthographic neighborhood effects for individual words (with and without homonyms). The simplest measure is based on Coltheart’s N (i.e., OrthoN; Coltheart et al.,1979), which is the number of alternate words that can be generated by changing a single letter in a word. For example, the word cat includes the orthographic neighbors oat, cot, vat, cab, chat, mat, cam, bat, rat, cad, hat, cap, pat, fat, flat, frat, sat, eat, car, cut, and can among others (Balota et al., 2007). 

The ELP database also reports the mean frequency of the orthographic neighbors per individual words (FreqO) and includes orthographic similarity effects based on Levenshtein distance (Levenshtein, 1966) per word. The measure, called orthographic Levenshtein distance 20 (OLD20, Yarkoni, Balota, & Yap, 2008) measures the minimum number of substitution, insertion, or deletion operations needed to change one word into another and calculates the mean LD for a word in relation to its 20 closest neighbors. 

Phonological neighbors. The ELP database also contains phonological neighborhood features based on phonemes. The ELP calculates the same features for phonological neighbors as it does for orthographic neighbors. These include measures based on Coltheart’s N (i.e., PhonoN), the mean frequency of the phonological neighbors per individual word (FreqP), and phonological neighbors based on Levenshtein distance (PLD). Counts are provided for all words and words that are not homonyms.

Rhyme counts. TAADA provides rhyme counts for each word in the Perfect Rhymes Dictionary (PeRDict; Crossley & Choi, 2024). PeRDict includes perfect rhymes (~48,000) based on rhyme patterns extracted from the CMUdict. Because many of the words in the CMUdict are rare and unlikely to influence decoding, PeRDict also provides perfect rhymes for the 1,000, 2,500, 5,000, and 10,000 most frequent words in the English language as identified by COCA.

Conditional probability counts. TAADA includes conditional probability counts for most words in the English language. The conditional probability counts come from Berndt et al. (1987), who calculated grapheme to phoneme/diphthong correspondences for all graphemes and grapheme clusters in English based on a 17,310 corpus of words. To do this, they identified all correspondences between a grapheme (or grapheme cluster) and a phoneme. They then added the frequencies of all these correspondences to derive a total frequency for each grapheme. Lastly, they calculated a reverse probability by dividing the frequency of each individual correspondences by the total frequency for the grapheme. In practice, the grapheme cluster OY-E as found in gargoyle has a one-to-one correspondence with the diphthong ɔɪ as does the grapheme cluster PP as found apple so the reverse probability would be 1. In contrast, the grapheme I has six different phonemic realization in English: ɪ as found in in (which has a reverse probability of .716), ə as found in civil (which has a reverse probability of .18), aɪ as found in find (which has a reverse probability of .074), ɜ as found in affirm (which has a reverse probability of .015), j as found in senior (which has a reverse probability of .008), and i as in ski (which has a reverse probability of .005). Berndt et al. also calculated the prior probabilities of graphemes by dividing the total frequency of a grapheme by the total number of occurrences of all graphemes within their word corpus.

From these counts, we used a Python script to calculate conditional probability counts for English words. The script used regular expressions to identify graphemes and grapheme clusters in words and then calculated probability counts for that word based on the minimum, maximum, and average reverse probability counts for each grapheme or grapheme cluster. In the case of the grapheme I above, the maximum count would be .716, the minimum count would be .005, and the mean count would be the average of all six possible phoneme/diphthong conditional probability counts. Weighted counts are also calculated by dividing the min, max, and mean scores by the number of phonemes in the word. Counts were calculated to vowels, consonants, and vowel + consonants.

Additionally, the script calculated the conditional probability counts for each word to derive their conditional probability. We first converted the phonemes in the Berndt dictionary into phonemes used in the CMU (e.g., ay to EY1, and ih to IH0, where the numerals demark primary/secondary or absence of stress). After finding some discrepancies between the CMU dictionary and the Berndt data (e.g., The phoneme IH2 is only represented as the grapheme cluster Y-E in CMU, while it can denote the grapheme cluster EA in Berndt’s data), we elected to remove the stress markers in the CMU dictionary to account for discrepancies. The script then mapped graphemes to phonemes using regular expressions, deriving conditional probability counts for each word. For example, the word tap can be broken down into three phonemes t, ae, and p, where the conditional probability of the grapheme T mapping to the phoneme t is .97 (i.e., P(t | T) = .97), and P(ae | A) .54, and P(p | P) = 1.00. In this example, the average conditional probability (the sum of the conditional probabilities divided by the number of graphemes) for the word tap is 0.837. This measure serves as the average predictability of the grapheme and phoneme pairs in a word.

There a few main scripts here

1. Script to calculate the majority of TAADA variable except Conditional Probability. This produces a dataframe for words in the English language and their decoding values
2. The script to calculate Conditional probability is at ....
3. Script to calculate mean scores for all TAADA variables for a corpus of texts. This includes
   a. The dataframe with all the decoding variable scores by word (decoding_1_dataframe.csv). This is the basis for TAADA.
   b. Python script to calculate mean decoding scores for words in the texts (calculate_TAADA_measures_from_texts_github.ipynb)


TAADA is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.: https://creativecommons.org/licenses/by-nc-sa/4.0/
