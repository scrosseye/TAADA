{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a2f46b5-a798-470d-bae0-3599ed5cb81e",
   "metadata": {},
   "source": [
    "This file will do the following\n",
    "\n",
    "1. Read in a corpus (CLEAR_corpus_final.csv)\n",
    "    - This corpus is available at https://github.com/scrosseye/CLEAR-Corpus\n",
    "2. Read in decoding values dataframe (decoding_1_dataframe.csv)\n",
    "    - This contains decoding values for words in the English language\n",
    "3. Calculate mean decoding values for texts in the corpus for\n",
    "    - Lemmatized and unlemmatized words\n",
    "    - Content words and all words (i.e., content and function words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b052560-d4d5-426a-8e63-8c29d05d3c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['calculate_decoding_measures_from_texts_spacy.ipynb', '~$key_decoder_variables_reference.xlsx', 'calculate_decoding_measures_from_texts_spacy_pipeline_optimized.ipynb', 'CLEAR_corpus_final.csv', '.DS_Store', 'Untitled.ipynb', 'key_decoder_variables_reference.xlsx', 'calculate_decoding_prac.ipynb', 'decoding_1_dataframe.csv', 'decoding_features_clear.csv', '.ipynb_checkpoints', 'CLEAR_corpus_prac_simple.csv', 'calculate_decoding_measures_from_texts_spacy_pipeline.ipynb']\n"
     ]
    }
   ],
   "source": [
    "#where are you?\n",
    "\n",
    "import os\n",
    "\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e57c29f3-2fb8-4613-959e-b553733262e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15717f46-2053-4f7c-9ff6-ee6c153edb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0            this is a simple practice text with ghlq.\n",
      "1                                   same, but simpler.\n",
      "2    words increasing, i think. about aaberg and aa...\n",
      "3                                          it will be.\n",
      "Name: Excerpt, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Author</th>\n",
       "      <th>Title</th>\n",
       "      <th>Anthology</th>\n",
       "      <th>URL</th>\n",
       "      <th>Pub Year</th>\n",
       "      <th>Categ</th>\n",
       "      <th>Sub Cat</th>\n",
       "      <th>Lexile Band</th>\n",
       "      <th>Location</th>\n",
       "      <th>...</th>\n",
       "      <th>BT_easiness</th>\n",
       "      <th>s.e.</th>\n",
       "      <th>Flesch-Reading-Ease</th>\n",
       "      <th>Flesch-Kincaid-Grade-Level</th>\n",
       "      <th>Automated Readability Index</th>\n",
       "      <th>SMOG Readability</th>\n",
       "      <th>New Dale-Chall Readability Formula</th>\n",
       "      <th>CAREC</th>\n",
       "      <th>CAREC_M</th>\n",
       "      <th>CML2RI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>400</td>\n",
       "      <td>Carolyn Wells</td>\n",
       "      <td>Patty's Suitors</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.gutenberg.org/cache/epub/5631/pg563...</td>\n",
       "      <td>1914</td>\n",
       "      <td>Lit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>900</td>\n",
       "      <td>mid</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "      <td>81.70</td>\n",
       "      <td>5.95</td>\n",
       "      <td>7.37</td>\n",
       "      <td>8</td>\n",
       "      <td>6.55</td>\n",
       "      <td>0.12102</td>\n",
       "      <td>0.11952</td>\n",
       "      <td>12.097815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>401</td>\n",
       "      <td>Carolyn Wells</td>\n",
       "      <td>Two Little Women on a Holiday</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.gutenberg.org/cache/epub/5893/pg589...</td>\n",
       "      <td>1917</td>\n",
       "      <td>Lit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>700</td>\n",
       "      <td>mid</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>0.480805</td>\n",
       "      <td>80.26</td>\n",
       "      <td>4.86</td>\n",
       "      <td>4.16</td>\n",
       "      <td>7</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.04921</td>\n",
       "      <td>0.04921</td>\n",
       "      <td>22.550179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>402</td>\n",
       "      <td>Carolyn Wells</td>\n",
       "      <td>Patty Blossom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.gutenberg.org/cache/epub/20945/pg20...</td>\n",
       "      <td>1917</td>\n",
       "      <td>Lit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>900</td>\n",
       "      <td>mid</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>0.476676</td>\n",
       "      <td>79.04</td>\n",
       "      <td>6.03</td>\n",
       "      <td>5.81</td>\n",
       "      <td>9</td>\n",
       "      <td>7.31</td>\n",
       "      <td>0.10172</td>\n",
       "      <td>0.09724</td>\n",
       "      <td>18.125279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>403</td>\n",
       "      <td>CHARLES KINGSLEY</td>\n",
       "      <td>THE WATER-BABIES\\nA Fairy Tale for a Land-Baby</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.gutenberg.org/files/25564/25564-h/2...</td>\n",
       "      <td>1863</td>\n",
       "      <td>Lit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1300</td>\n",
       "      <td>mid</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.785965</td>\n",
       "      <td>0.526599</td>\n",
       "      <td>44.77</td>\n",
       "      <td>20.51</td>\n",
       "      <td>24.87</td>\n",
       "      <td>12</td>\n",
       "      <td>8.56</td>\n",
       "      <td>0.07491</td>\n",
       "      <td>0.08856</td>\n",
       "      <td>10.959460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID            Author                                           Title  \\\n",
       "0  400     Carolyn Wells                                 Patty's Suitors   \n",
       "1  401     Carolyn Wells                   Two Little Women on a Holiday   \n",
       "2  402     Carolyn Wells                                   Patty Blossom   \n",
       "3  403  CHARLES KINGSLEY  THE WATER-BABIES\\nA Fairy Tale for a Land-Baby   \n",
       "\n",
       "   Anthology                                                URL  Pub Year  \\\n",
       "0        NaN  http://www.gutenberg.org/cache/epub/5631/pg563...      1914   \n",
       "1        NaN  http://www.gutenberg.org/cache/epub/5893/pg589...      1917   \n",
       "2        NaN  http://www.gutenberg.org/cache/epub/20945/pg20...      1917   \n",
       "3        NaN  http://www.gutenberg.org/files/25564/25564-h/2...      1863   \n",
       "\n",
       "  Categ  Sub Cat  Lexile Band Location  ...  BT_easiness      s.e.  \\\n",
       "0   Lit      NaN          900      mid  ...    -0.340259  0.464009   \n",
       "1   Lit      NaN          700      mid  ...    -0.315372  0.480805   \n",
       "2   Lit      NaN          900      mid  ...    -0.580118  0.476676   \n",
       "3   Lit      NaN         1300      mid  ...    -1.785965  0.526599   \n",
       "\n",
       "   Flesch-Reading-Ease  Flesch-Kincaid-Grade-Level  \\\n",
       "0                81.70                        5.95   \n",
       "1                80.26                        4.86   \n",
       "2                79.04                        6.03   \n",
       "3                44.77                       20.51   \n",
       "\n",
       "  Automated Readability Index  SMOG Readability  \\\n",
       "0                        7.37                 8   \n",
       "1                        4.16                 7   \n",
       "2                        5.81                 9   \n",
       "3                       24.87                12   \n",
       "\n",
       "   New Dale-Chall Readability Formula    CAREC  CAREC_M     CML2RI  \n",
       "0                                6.55  0.12102  0.11952  12.097815  \n",
       "1                                6.25  0.04921  0.04921  22.550179  \n",
       "2                                7.31  0.10172  0.09724  18.125279  \n",
       "3                                8.56  0.07491  0.08856  10.959460  \n",
       "\n",
       "[4 rows x 28 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import texts for analysis\n",
    "#in this case, the clear corpus\n",
    "\n",
    "df = pd.read_csv('CLEAR_corpus_prac_simple.csv') #prac texts\n",
    "#df = pd.read_csv('CLEAR_corpus_final.csv') #final texts\n",
    "\n",
    "#lower case everything\n",
    "df['Excerpt'] = df['Excerpt'].str.lower() #let's lower case everything in the text column of the Pandas dataframe\n",
    "\n",
    "print(df['Excerpt']) #lowercase works?\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb22c71f-85f0-4cb8-ba78-54b34dd08023",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5p/zd2tqnh90kg4fnzxtpkpx2yh0000gn/T/ipykernel_52066/245165305.py:4: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  decoding = pd.read_csv ('decoding_1_dataframe.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 121751 entries, 0 to 121750\n",
      "Data columns (total 53 columns):\n",
      " #   Column                                 Non-Null Count   Dtype  \n",
      "---  ------                                 --------------   -----  \n",
      " 0   words_lower                            121750 non-null  object \n",
      " 1   num_syllables                          121751 non-null  int64  \n",
      " 2   num_letters                            121751 non-null  int64  \n",
      " 3   num_phonemes                           121751 non-null  int64  \n",
      " 4   discrepancy_raw                        121751 non-null  int64  \n",
      " 5   discrepancy_ratio                      121751 non-null  float64\n",
      " 6   avg_syllable_length                    121751 non-null  float64\n",
      " 7   num_consonants_characters              121751 non-null  int64  \n",
      " 8   num_vowel_characters                   121751 non-null  int64  \n",
      " 9   num_consonants_phonemes                121751 non-null  int64  \n",
      " 10  num_vowel_phonemes                     121751 non-null  int64  \n",
      " 11  avg_phonemes_per_character_consonants  121751 non-null  float64\n",
      " 12  avg_phonemes_per_character_vowels      121751 non-null  float64\n",
      " 13  avg_phonemes_per_character_all         121751 non-null  float64\n",
      " 14  prior_prob_cons                        121751 non-null  float64\n",
      " 15  max_prob_cons                          121751 non-null  float64\n",
      " 16  min_prob_cons                          121751 non-null  float64\n",
      " 17  mid_prob_cons                          121751 non-null  float64\n",
      " 18  number_phonemes_cons                   121751 non-null  float64\n",
      " 19  prior_prob_vowel                       121751 non-null  float64\n",
      " 20  max_prob_vowel                         121751 non-null  float64\n",
      " 21  min_prob_vowel                         121751 non-null  float64\n",
      " 22  mid_prob_vowel                         121751 non-null  float64\n",
      " 23  number_phonemes_vowel                  121751 non-null  float64\n",
      " 24  prior_prob_all                         121751 non-null  float64\n",
      " 25  max_prob_all                           121751 non-null  float64\n",
      " 26  mid_prob_all                           121751 non-null  float64\n",
      " 27  min_prob_all                           121751 non-null  float64\n",
      " 28  number_phonemes_all                    121751 non-null  float64\n",
      " 29  Conditional_Probability_Average        121741 non-null  float64\n",
      " 30  Ortho_N                                47112 non-null   float64\n",
      " 31  Phono_N                                47112 non-null   float64\n",
      " 32  Phono_N_H                              47112 non-null   float64\n",
      " 33  OG_N                                   47112 non-null   float64\n",
      " 34  OG_N_H                                 16326 non-null   float64\n",
      " 35  Freq_N                                 23567 non-null   float64\n",
      " 36  Freq_N_P                               25541 non-null   float64\n",
      " 37  Freq_N_PH                              26248 non-null   float64\n",
      " 38  Freq_N_OG                              15885 non-null   float64\n",
      " 39  Freq_N_OGH                             16326 non-null   float64\n",
      " 40  OLD                                    33601 non-null   float64\n",
      " 41  OLDF                                   33601 non-null   float64\n",
      " 42  PLD                                    33601 non-null   float64\n",
      " 43  PLDF                                   33601 non-null   float64\n",
      " 44  subtlexus_log_freq                     48615 non-null   float64\n",
      " 45  subtlexus_log_cd                       48615 non-null   float64\n",
      " 46  coca_maga_cd                           66693 non-null   float64\n",
      " 47  coca_mag_log_freq                      66693 non-null   float64\n",
      " 48  num_rhymes_full_elp                    46792 non-null   float64\n",
      " 49  num_rhymes_1000_coca                   46792 non-null   float64\n",
      " 50  num_rhymes_2500_coca                   46792 non-null   float64\n",
      " 51  num_rhymes_5000_coca                   46792 non-null   float64\n",
      " 52  num_rhymes_10000_coca                  46792 non-null   float64\n",
      "dtypes: float64(44), int64(8), object(1)\n",
      "memory usage: 49.2+ MB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#call in dataframe and convert to dictionary\n",
    "\n",
    "decoding = pd.read_csv ('decoding_1_dataframe.csv')\n",
    "\n",
    "decoding_2 = decoding.iloc[:, np.r_[1, 4:56]] #use numpy to call non-consecutive columns\n",
    "\n",
    "#print(decoding_2[:10])\n",
    "#print(decoding_2.size)\n",
    "#print(decoding_2.shape)\n",
    "decoding_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a25ebc-c7fc-4dda-b6d6-7220c84664a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#just to look at the data\n",
    "\n",
    "#decoding_2.sort_vals('discrepancy_ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358cd5ee-b15a-4d43-9c8c-43127a4d8c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sets key to word, transforms data, changes to dictionary\n",
    "decoding_dic = decoding_2.set_index('words_lower').T.to_dict('list') #make dictionary key and vals (as a list). This makes it easier to call vals in slices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2f03c31-8a1c-4b01-9d82-4f323fe5d2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fast\n",
      "[1.0, 4.0, 4.0, 0.0, 1.0, 4.0, 3.0, 1.0, 3.0, 1.0, 1.0, 1.0, 1.0, 0.00745, 0.999, 0.5005, 0.74975, 1.5, 0.0712, 0.542, 0.0005, 0.27125, 8.0, 0.039325, 0.7705, 0.5105, 0.2505, 4.75, 0.84525, 13.0, 21.0, 21.0, 9.0, 9.39, 8.47, 8.04, 8.04, 9.39, 9.39, 1.3, 8.57, 1.05, 8.97, 3.8458, 3.5808, 0.146622053, 4.102074424, 23.0, 2.0, 4.0, 5.0, 5.0]\n",
      "a\n",
      "[1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0712, 0.542, 0.0005, 0.27125, 8.0, 0.0356, 0.271, 0.135625, 0.00025, 4.0, 0.0005, 5.0, 15.0, 17.0, 3.0, 13.65, 13.04, 9.3, 9.39, 13.65, 13.65, 1.45, 9.83, 1.0, 10.16, 6.0175, 3.9234, 0.998571375, 6.395114071, nan, nan, nan, nan, nan]\n",
      "as\n",
      "[1.0, 2.0, 2.0, 0.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0488, 0.868, 0.003, 0.4355, 4.0, 0.0712, 0.542, 0.0005, 0.27125, 8.0, 0.06, 0.705, 0.353375, 0.00175, 6.0, 0.1245, 16.0, 27.0, 27.0, 6.0, 13.25, 10.87, 8.82, 8.82, 13.25, 13.25, 1.1, 9.17, 1.0, 11.89, 5.0533, 3.9159, 0.947234858, 5.790090624, 3.0, 0.0, 0.0, 0.0, 0.0]\n",
      "a42128\n",
      "[6.0, 6.0, 13.0, 7.0, 2.166666667, 1.0, 5.0, 1.0, 7.0, 6.0, 0.714285714, 0.166666667, 0.461538462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0712, 0.542, 0.0005, 0.27125, 8.0, 0.0356, 0.271, 0.135625, 0.00025, 4.0, 0.129, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "aa\n",
      "[2.0, 2.0, 2.0, 0.0, 1.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0712, 0.542, 0.0005, 0.27125, 8.0, 0.0356, 0.271, 0.135625, 0.00025, 4.0, 0.129, 24.0, 25.0, 25.0, 10.0, 10.32, 10.76, 9.45, 9.45, 10.32, 10.32, nan, nan, nan, nan, 1.9445, 1.8513, 0.006034061, 2.852373735, nan, nan, nan, nan, nan]\n",
      "aaa\n",
      "[3.0, 3.0, 7.0, 4.0, 2.333333333, 1.0, 0.0, 3.0, 4.0, 3.0, 0.0, 1.0, 0.428571429, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0712, 0.542, 0.0005, 0.27125, 8.0, 0.0356, 0.271, 0.135625, 0.00025, 4.0, 0.129, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.415, 1.3802, 0.003383586, 2.518485208, nan, nan, nan, nan, nan]\n",
      "aaberg\n",
      "[2.0, 6.0, 4.0, -2.0, 0.666666667, 3.0, 3.0, 3.0, 2.0, 2.0, 1.5, 1.5, 1.5, 0.040533333, 0.88, 0.669333333, 0.774666667, 1.666666667, 0.0718, 0.501, 0.000366667, 0.250683333, 8.0, 0.056166667, 0.6905, 0.512675, 0.33485, 4.833333333, 0.82, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "aachen\n",
      "[2.0, 6.0, 4.0, -2.0, 0.666666667, 3.0, 3.0, 3.0, 2.0, 2.0, 1.5, 1.5, 1.5, 0.002285, 0.82, 0.5345, 0.67725, 2.0, 0.0712, 0.542, 0.0005, 0.27125, 8.0, 0.0367425, 0.681, 0.47425, 0.2675, 5.0, 0.451, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.000150382, 1.188942056, nan, nan, nan, nan, nan]\n"
     ]
    }
   ],
   "source": [
    "####IGNORE\n",
    "\n",
    "#what does the dictionary look like?\n",
    "\n",
    "#check out single word\n",
    "for key, val in decoding_dic.items():\n",
    "  if key == \"fast\":\n",
    "    print(key)\n",
    "    print(val)\n",
    "\n",
    "\n",
    "#check out five words\n",
    "c = 0\n",
    "\n",
    "for key, val in decoding_dic.items():\n",
    "  print(key)\n",
    "  print(val)\n",
    "  c += 1\n",
    "  if c > 6:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefec843-8843-41ea-9d42-324c3688e68d",
   "metadata": {},
   "source": [
    "**THIS IS THE MAIN CODE FOR UNLEMMATIZED DATA**\n",
    "\n",
    "        - All words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e1d84040-b4df-4e54-ba17-9f1f3aaf833c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011073827743530273\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "df_docs = list(nlp.pipe(df.Excerpt)) #call in nlp pipeline to spacy all excerpt rows\n",
    "\n",
    "nw_token_text = []\n",
    "num_syllables_token_aw_text = []\n",
    "num_letters_token_aw_text = []\n",
    "num_phonemes_token_aw_text = []\n",
    "discrepancy_raw_token_aw_text = []\n",
    "discrepancy_ratio_token_aw_text = []\n",
    "avg_syllable_length_token_aw_text = []\n",
    "num_consonants_characters_token_aw_text = []\n",
    "num_vowel_characters_token_aw_text = []\n",
    "num_consonants_phonemes_token_aw_text = []\n",
    "num_vowel_phonemes_token_aw_text = []\n",
    "avg_phonemes_per_character_consonants_token_aw_text = []\n",
    "avg_phonemes_per_character_vowels_token_aw_text = []\n",
    "avg_phonemes_per_character_all_token_aw_text = []\n",
    "prior_prob_cons_token_aw_text = []\n",
    "max_prob_cons_token_aw_text = []\n",
    "min_prob_cons_token_aw_text = []\n",
    "mid_prob_cons_token_aw_text = []\n",
    "number_phonemes_cons_token_aw_text = []\n",
    "prior_prob_vowel_token_aw_text = []\n",
    "max_prob_vowel_token_aw_text = []\n",
    "min_prob_vowel_token_aw_text = []\n",
    "mid_prob_vowel_token_aw_text = []\n",
    "number_phonemes_vowel_token_aw_text = []\n",
    "prior_prob_all_token_aw_text = []\n",
    "max_prob_all_token_aw_text = []\n",
    "mid_prob_all_token_aw_text = []\n",
    "min_prob_all_token_aw_text = []\n",
    "number_phonemes_all_token_aw_text = []\n",
    "Conditional_Probability_Average_token_aw_text = []\n",
    "Ortho_N_token_aw_text = []\n",
    "Phono_N_token_aw_text = []\n",
    "Phono_N_H_token_aw_text = []\n",
    "OG_N_token_aw_text = []\n",
    "OG_N_H_token_aw_text = []\n",
    "Freq_N_token_aw_text = []\n",
    "Freq_N_P_token_aw_text = []\n",
    "Freq_N_PH_token_aw_text = []\n",
    "Freq_N_OG_token_aw_text = []\n",
    "Freq_N_OGH_token_aw_text = []\n",
    "OLD_token_aw_text = []\n",
    "OLDF_token_aw_text = []\n",
    "PLD_token_aw_text = []\n",
    "PLDF_token_aw_text = []\n",
    "subtlexus_log_freq_token_aw_text = []\n",
    "subtlexus_log_cd_token_aw_text = []\n",
    "coca_maga_cd_token_aw_text = []\n",
    "coca_mag_log_freq_token_aw_text = []\n",
    "num_rhymes_full_elp_token_aw_text = []\n",
    "num_rhymes_1000_coca_token_aw_text = []\n",
    "num_rhymes_2500_coca_token_aw_text = []\n",
    "num_rhymes_5000_coca_token_aw_text = []\n",
    "num_rhymes_10000_coca_token_aw_text = []\n",
    "\n",
    "\n",
    "\n",
    "for tokenized_doc in df_docs:\n",
    "    #print(text)\n",
    "    nw_token = []\n",
    "    num_syllables_token_aw = []\n",
    "    num_letters_token_aw = []\n",
    "    num_phonemes_token_aw = []\n",
    "    discrepancy_raw_token_aw = []\n",
    "    discrepancy_ratio_token_aw = []\n",
    "    avg_syllable_length_token_aw = []\n",
    "    num_consonants_characters_token_aw = []\n",
    "    num_vowel_characters_token_aw = []\n",
    "    num_consonants_phonemes_token_aw = []\n",
    "    num_vowel_phonemes_token_aw = []\n",
    "    avg_phonemes_per_character_consonants_token_aw = []\n",
    "    avg_phonemes_per_character_vowels_token_aw = []\n",
    "    avg_phonemes_per_character_all_token_aw = []\n",
    "    prior_prob_cons_token_aw = []\n",
    "    max_prob_cons_token_aw = []\n",
    "    min_prob_cons_token_aw = []\n",
    "    mid_prob_cons_token_aw = []\n",
    "    number_phonemes_cons_token_aw = []\n",
    "    prior_prob_vowel_token_aw = []\n",
    "    max_prob_vowel_token_aw = []\n",
    "    min_prob_vowel_token_aw = []\n",
    "    mid_prob_vowel_token_aw = []\n",
    "    number_phonemes_vowel_token_aw = []\n",
    "    prior_prob_all_token_aw = []\n",
    "    max_prob_all_token_aw = []\n",
    "    mid_prob_all_token_aw = []\n",
    "    min_prob_all_token_aw = []\n",
    "    number_phonemes_all_token_aw = []\n",
    "    Conditional_Probability_Average_token_aw = []\n",
    "    Ortho_N_token_aw = []\n",
    "    Phono_N_token_aw = []\n",
    "    Phono_N_H_token_aw = []\n",
    "    OG_N_token_aw = []\n",
    "    OG_N_H_token_aw = []\n",
    "    Freq_N_token_aw = []\n",
    "    Freq_N_P_token_aw = []\n",
    "    Freq_N_PH_token_aw = []\n",
    "    Freq_N_OG_token_aw = []\n",
    "    Freq_N_OGH_token_aw = []\n",
    "    OLD_token_aw = []\n",
    "    OLDF_token_aw = []\n",
    "    PLD_token_aw = []\n",
    "    PLDF_token_aw = []\n",
    "    subtlexus_log_freq_token_aw = []\n",
    "    subtlexus_log_cd_token_aw = []\n",
    "    coca_maga_cd_token_aw = []\n",
    "    coca_mag_log_freq_token_aw = []\n",
    "    num_rhymes_full_elp_token_aw = []\n",
    "    num_rhymes_1000_coca_token_aw = []\n",
    "    num_rhymes_2500_coca_token_aw = []\n",
    "    num_rhymes_5000_coca_token_aw = []\n",
    "    num_rhymes_10000_coca_token_aw = []\n",
    "#    print(text)\n",
    "    for token in tokenized_doc:\n",
    "        #print(token.text)\n",
    "        if token.is_alpha: #remove all non-alpha crap\n",
    "            #print(token.text)\n",
    "            nw_token.append(str(token))\n",
    "            try: \n",
    "            #try block encloses the code that might raise an exception (in this case, a KeyError because the val is not there)\n",
    "            #it executes the code inside the block\n",
    "                val = decoding_dic[token.text]\n",
    "                #retrieve the value associated with the token\n",
    "                num_syllables_token_aw.append(val[0])\n",
    "                num_letters_token_aw.append(val[1])\n",
    "                num_phonemes_token_aw.append(val[2])\n",
    "                discrepancy_raw_token_aw.append(val[3])\n",
    "                discrepancy_ratio_token_aw.append(val[4])\n",
    "                avg_syllable_length_token_aw.append(val[5])\n",
    "                num_consonants_characters_token_aw.append(val[6])\n",
    "                num_vowel_characters_token_aw.append(val[7])\n",
    "                num_consonants_phonemes_token_aw.append(val[8])\n",
    "                num_vowel_phonemes_token_aw.append(val[9])\n",
    "                avg_phonemes_per_character_consonants_token_aw.append(val[10])\n",
    "                avg_phonemes_per_character_vowels_token_aw.append(val[11])\n",
    "                avg_phonemes_per_character_all_token_aw.append(val[12])\n",
    "                prior_prob_cons_token_aw.append(val[13])\n",
    "                max_prob_cons_token_aw.append(val[14])\n",
    "                min_prob_cons_token_aw.append(val[15])\n",
    "                mid_prob_cons_token_aw.append(val[16])\n",
    "                number_phonemes_cons_token_aw.append(val[17])\n",
    "                prior_prob_vowel_token_aw.append(val[18])\n",
    "                max_prob_vowel_token_aw.append(val[19])\n",
    "                min_prob_vowel_token_aw.append(val[20])\n",
    "                mid_prob_vowel_token_aw.append(val[21])\n",
    "                number_phonemes_vowel_token_aw.append(val[22])\n",
    "                prior_prob_all_token_aw.append(val[23])\n",
    "                max_prob_all_token_aw.append(val[24])\n",
    "                mid_prob_all_token_aw.append(val[25])\n",
    "                min_prob_all_token_aw.append(val[26])\n",
    "                number_phonemes_all_token_aw.append(val[27])\n",
    "                Conditional_Probability_Average_token_aw.append(val[28])\n",
    "                Ortho_N_token_aw.append(val[29])\n",
    "                Phono_N_token_aw.append(val[30])\n",
    "                Phono_N_H_token_aw.append(val[31])\n",
    "                OG_N_token_aw.append(val[32])\n",
    "                OG_N_H_token_aw.append(val[33])\n",
    "                Freq_N_token_aw.append(val[34])\n",
    "                Freq_N_P_token_aw.append(val[35])\n",
    "                Freq_N_PH_token_aw.append(val[36])\n",
    "                Freq_N_OG_token_aw.append(val[37])\n",
    "                Freq_N_OGH_token_aw.append(val[38])\n",
    "                OLD_token_aw.append(val[39])\n",
    "                OLDF_token_aw.append(val[40])\n",
    "                PLD_token_aw.append(val[41])\n",
    "                PLDF_token_aw.append(val[42])\n",
    "                subtlexus_log_freq_token_aw.append(val[43])\n",
    "                subtlexus_log_cd_token_aw.append(val[44])\n",
    "                coca_maga_cd_token_aw.append(val[45])\n",
    "                coca_mag_log_freq_token_aw.append(val[46])\n",
    "                num_rhymes_full_elp_token_aw.append(val[47])\n",
    "                num_rhymes_1000_coca_token_aw.append(val[48])\n",
    "                num_rhymes_2500_coca_token_aw.append(val[49])\n",
    "                num_rhymes_5000_coca_token_aw.append(val[50])\n",
    "                num_rhymes_10000_coca_token_aw.append(val[51])\n",
    "            except:\n",
    "            #The except block is executed if a KeyError occurs \n",
    "            #when the specified key is not found in the dictionary.\n",
    "                pass\n",
    "                #if a keyerror occurs, just ignore it.\n",
    "                \n",
    "    nw_token_text.append(len(nw_token))\n",
    "    num_syllables_token_aw_text.append(num_syllables_token_aw)\n",
    "    num_letters_token_aw_text.append(num_letters_token_aw)\n",
    "    num_phonemes_token_aw_text.append(num_phonemes_token_aw)\n",
    "    discrepancy_raw_token_aw_text.append(discrepancy_raw_token_aw)\n",
    "    discrepancy_ratio_token_aw_text.append(discrepancy_ratio_token_aw)\n",
    "    avg_syllable_length_token_aw_text.append(avg_syllable_length_token_aw)\n",
    "    num_consonants_characters_token_aw_text.append(num_consonants_characters_token_aw)\n",
    "    num_vowel_characters_token_aw_text.append(num_vowel_characters_token_aw)\n",
    "    num_consonants_phonemes_token_aw_text.append(num_consonants_phonemes_token_aw)\n",
    "    num_vowel_phonemes_token_aw_text.append(num_vowel_phonemes_token_aw)\n",
    "    avg_phonemes_per_character_consonants_token_aw_text.append(avg_phonemes_per_character_consonants_token_aw)\n",
    "    avg_phonemes_per_character_vowels_token_aw_text.append(avg_phonemes_per_character_vowels_token_aw)\n",
    "    avg_phonemes_per_character_all_token_aw_text.append(avg_phonemes_per_character_all_token_aw)\n",
    "    prior_prob_cons_token_aw_text.append(prior_prob_cons_token_aw)\n",
    "    max_prob_cons_token_aw_text.append(max_prob_cons_token_aw)\n",
    "    min_prob_cons_token_aw_text.append(min_prob_cons_token_aw)\n",
    "    mid_prob_cons_token_aw_text.append(mid_prob_cons_token_aw)\n",
    "    number_phonemes_cons_token_aw_text.append(number_phonemes_cons_token_aw)\n",
    "    prior_prob_vowel_token_aw_text.append(prior_prob_vowel_token_aw)\n",
    "    max_prob_vowel_token_aw_text.append(max_prob_vowel_token_aw)\n",
    "    min_prob_vowel_token_aw_text.append(min_prob_vowel_token_aw)\n",
    "    mid_prob_vowel_token_aw_text.append(mid_prob_vowel_token_aw)\n",
    "    number_phonemes_vowel_token_aw_text.append(number_phonemes_vowel_token_aw)\n",
    "    prior_prob_all_token_aw_text.append(prior_prob_all_token_aw)\n",
    "    max_prob_all_token_aw_text.append(max_prob_all_token_aw)\n",
    "    mid_prob_all_token_aw_text.append(mid_prob_all_token_aw)\n",
    "    min_prob_all_token_aw_text.append(min_prob_all_token_aw)\n",
    "    number_phonemes_all_token_aw_text.append(number_phonemes_all_token_aw)\n",
    "    Conditional_Probability_Average_token_aw_text.append(Conditional_Probability_Average_token_aw)\n",
    "    Ortho_N_token_aw_text.append(Ortho_N_token_aw)\n",
    "    Phono_N_token_aw_text.append(Phono_N_token_aw)\n",
    "    Phono_N_H_token_aw_text.append(Phono_N_H_token_aw)\n",
    "    OG_N_token_aw_text.append(OG_N_token_aw)\n",
    "    OG_N_H_token_aw_text.append(OG_N_H_token_aw)\n",
    "    Freq_N_token_aw_text.append(Freq_N_token_aw)\n",
    "    Freq_N_P_token_aw_text.append(Freq_N_P_token_aw)\n",
    "    Freq_N_PH_token_aw_text.append(Freq_N_PH_token_aw)\n",
    "    Freq_N_OG_token_aw_text.append(Freq_N_OG_token_aw)\n",
    "    Freq_N_OGH_token_aw_text.append(Freq_N_OGH_token_aw)\n",
    "    OLD_token_aw_text.append(OLD_token_aw)\n",
    "    OLDF_token_aw_text.append(OLDF_token_aw)\n",
    "    PLD_token_aw_text.append(PLD_token_aw)\n",
    "    PLDF_token_aw_text.append(PLDF_token_aw)\n",
    "    subtlexus_log_freq_token_aw_text.append(subtlexus_log_freq_token_aw)\n",
    "    subtlexus_log_cd_token_aw_text.append(subtlexus_log_cd_token_aw)\n",
    "    coca_maga_cd_token_aw_text.append(coca_maga_cd_token_aw)\n",
    "    coca_mag_log_freq_token_aw_text.append(coca_mag_log_freq_token_aw)\n",
    "    num_rhymes_full_elp_token_aw_text.append(num_rhymes_full_elp_token_aw)\n",
    "    num_rhymes_1000_coca_token_aw_text.append(num_rhymes_1000_coca_token_aw)\n",
    "    num_rhymes_2500_coca_token_aw_text.append(num_rhymes_2500_coca_token_aw)\n",
    "    num_rhymes_5000_coca_token_aw_text.append(num_rhymes_5000_coca_token_aw)\n",
    "    num_rhymes_10000_coca_token_aw_text.append(num_rhymes_10000_coca_token_aw)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a053a696-4e05-4bb9-9fce-3aa5eafd8c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the length of the texts by number words [8, 3, 9, 3]\n",
      "this is a list of number of syllables for each word in the text [[1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0], [1.0, 1.0, 3.0], [1.0, 3.0, 1.0, 1.0, 2.0, 2.0, 1.0, 3.0, 3.0], [1.0, 1.0, 1.0]]\n",
      "this is a list of number of letters for each word in the text [[4.0, 2.0, 1.0, 6.0, 8.0, 4.0, 4.0], [4.0, 3.0, 7.0], [5.0, 10.0, 1.0, 5.0, 5.0, 6.0, 3.0, 3.0, 7.0], [2.0, 4.0, 2.0]]\n",
      "this is a list of number_phonemes_all_token_text for each word in the text [[4.75, 5.0, 4.0, 3.875, 4.0, 5.333333333, 4.0], [5.0, 5.5, 4.4], [5.5, 4.1, 3.0, 4.0, 5.0, 4.833333333, 5.0, 4.0, 4.5], [4.5, 3.5, 4.5]]\n",
      "this is a list of num_rhymes_full_elp_token_aw_text for each word in the text [[18.0, 8.0, nan, 2.0, 1.0, 5.0, 0.0], [28.0, 20.0, 0.0], [6.0, 10.0, 77.0, 21.0, 23.0, nan, nan, nan, 4.0], [41.0, 46.0, 113.0]]\n",
      "this is a list of PLDF_token_aw for each word in the text [[9.19, 10.46, 10.16, 7.21, 7.13, 8.0, 8.87], [10.05, 8.54, 7.56], [8.22, 6.98, 10.39, 8.75, 7.35, nan, 9.73, nan, 5.93], [10.3, 8.5, 9.87]]\n",
      "this is a list of Conditional_Probability_Average_token_aw_text for each word in the text [[0.616333333, 0.097, 0.0005, 0.911, 0.694571429, 0.696, 0.660333333], [0.83, 0.691666667, 0.911], [0.5265, 0.825, 0.074, 0.62, 0.574375, 0.82, 0.652833333, 0.129, 0.7222], [0.8445, 0.905333333, 0.615]]\n"
     ]
    }
   ],
   "source": [
    "#sanity checks\n",
    "\n",
    "print(f'this is the length of the texts by number words {nw_token_text[0:5]}') \n",
    "print(f'this is a list of number of syllables for each word in the text {num_syllables_token_aw_text}')\n",
    "print(f'this is a list of number of letters for each word in the text {num_letters_token_aw_text}')\n",
    "print(f'this is a list of number_phonemes_all_token_text for each word in the text {number_phonemes_all_token_aw_text}')\n",
    "print(f'this is a list of num_rhymes_full_elp_token_aw_text for each word in the text {num_rhymes_full_elp_token_aw_text}')\n",
    "print(f'this is a list of PLDF_token_aw for each word in the text {PLDF_token_aw_text}')\n",
    "print(f'this is a list of Conditional_Probability_Average_token_aw_text for each word in the text {Conditional_Probability_Average_token_aw_text}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8abd5d8d-7719-4c44-8f96-6fdef8550180",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove nan's from the list of lists. This is only needed for variables from Conditional_Probability_Average to num_rhymes_10000_coca\n",
    "#all the other variables have no nan's\n",
    "\n",
    "\n",
    "def remove_nan(list_of_lists):\n",
    "    return [\n",
    "        [val for val in sublist if val == val]\n",
    "        for sublist in list_of_lists\n",
    "    ]\n",
    "\n",
    "Conditional_Probability_Average_token_aw_text_no_nan = remove_nan(Conditional_Probability_Average_token_aw_text)\n",
    "Ortho_N_token_aw_text_no_nan = remove_nan(Ortho_N_token_aw_text)\n",
    "Phono_N_token_aw_text_no_nan = remove_nan(Phono_N_token_aw_text)\n",
    "Phono_N_H_token_aw_text_no_nan = remove_nan(Phono_N_H_token_aw_text)\n",
    "OG_N_token_aw_text_no_nan = remove_nan(OG_N_token_aw_text)\n",
    "OG_N_H_token_aw_text_no_nan = remove_nan(OG_N_H_token_aw_text)\n",
    "Freq_N_token_aw_text_no_nan = remove_nan(Freq_N_token_aw_text)\n",
    "Freq_N_P_token_aw_text_no_nan = remove_nan(Freq_N_P_token_aw_text)\n",
    "Freq_N_PH_token_aw_text_no_nan = remove_nan(Freq_N_PH_token_aw_text)\n",
    "Freq_N_OG_token_aw_text_no_nan = remove_nan(Freq_N_OG_token_aw_text)\n",
    "Freq_N_OGH_token_aw_text_no_nan = remove_nan(Freq_N_OGH_token_aw_text)\n",
    "OLD_token_aw_text_no_nan = remove_nan(OLD_token_aw_text)\n",
    "OLDF_token_aw_text_no_nan = remove_nan(OLDF_token_aw_text)\n",
    "PLD_token_aw_text_no_nan = remove_nan(PLD_token_aw_text)\n",
    "PLDF_token_aw_text_no_nan = remove_nan(PLDF_token_aw_text)\n",
    "subtlexus_log_freq_token_aw_text_no_nan = remove_nan(subtlexus_log_freq_token_aw_text)\n",
    "subtlexus_log_cd_token_aw_text_no_nan = remove_nan(subtlexus_log_cd_token_aw_text)\n",
    "coca_maga_cd_token_aw_text_no_nan = remove_nan(coca_maga_cd_token_aw_text)\n",
    "coca_mag_log_freq_token_aw_text_no_nan = remove_nan(coca_mag_log_freq_token_aw_text)\n",
    "num_rhymes_full_elp_token_aw_text_no_nan = remove_nan(num_rhymes_full_elp_token_aw_text)\n",
    "num_rhymes_1000_coca_token_aw_text_no_nan = remove_nan(num_rhymes_1000_coca_token_aw_text)\n",
    "num_rhymes_2500_coca_token_aw_text_no_nan = remove_nan(num_rhymes_2500_coca_token_aw_text)\n",
    "num_rhymes_5000_coca_token_aw_text_no_nan = remove_nan(num_rhymes_5000_coca_token_aw_text)\n",
    "num_rhymes_10000_coca_token_aw_text_no_nan = remove_nan(num_rhymes_10000_coca_token_aw_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5e4682b4-d6af-4585-9a5f-30c4fac46897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.19, 10.46, 10.16, 7.21, 7.13, 8.0, 8.87]]\n"
     ]
    }
   ],
   "source": [
    "#sanity check\n",
    "\n",
    "print(PLDF_token_aw_text_no_nan[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7a87f736-87c1-4754-81c2-882526a13ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#get lists that are average of sublists. If a sublist reports no items in the list, it will be given a 0\n",
    "\n",
    "num_syl_tok_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_syllables_token_aw_text] #if it is a sublist, get average, else (if it is not a sublist, empty list, return 0)\n",
    "num_let_tok_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_letters_token_aw_text]\n",
    "num_phone_tok_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_phonemes_token_aw_text]\n",
    "discrepancy_raw_tok_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in discrepancy_raw_token_aw_text]\n",
    "discrepancy_ratio_tok_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in discrepancy_ratio_token_aw_text]\n",
    "avg_syl_length_tok_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in avg_syllable_length_token_aw_text]\n",
    "num_cons_char_tok_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_consonants_characters_token_aw_text]\n",
    "num_vowel_char_tok_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_vowel_characters_token_aw_text]\n",
    "num_cons_phone_tok_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_consonants_phonemes_token_aw_text]\n",
    "num_vowel_phone_tok_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_vowel_phonemes_token_aw_text]\n",
    "avg_phone_per_char_cons_tok_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in avg_phonemes_per_character_consonants_token_aw_text]\n",
    "avg_phone_per_char_vowel_tok_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in avg_phonemes_per_character_vowels_token_aw_text]\n",
    "avg_phone_per_char_all_tok_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in avg_phonemes_per_character_all_token_aw_text]\n",
    "prior_prob_cons_tok_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in prior_prob_cons_token_aw_text]\n",
    "max_prob_cons_tok_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in max_prob_cons_token_aw_text]\n",
    "min_prob_cons_tok_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in min_prob_cons_token_aw_text]\n",
    "mid_prob_cons_tok_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in mid_prob_cons_token_aw_text]\n",
    "number_phone_cons_tok_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in number_phonemes_cons_token_aw_text]\n",
    "prior_prob_vowel_tok_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in prior_prob_vowel_token_aw_text]\n",
    "max_prob_vowel_tok_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in max_prob_vowel_token_aw_text]\n",
    "min_prob_vowel_tok_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in min_prob_vowel_token_aw_text]\n",
    "mid_prob_vowel_tok_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in mid_prob_vowel_token_aw_text]\n",
    "number_phone_vowel_tok_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in number_phonemes_vowel_token_aw_text]\n",
    "prior_prob_all_tok_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in prior_prob_all_token_aw_text]\n",
    "max_prob_all_tok_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in max_prob_all_token_aw_text]\n",
    "mid_prob_all_tok_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in mid_prob_all_token_aw_text]\n",
    "min_prob_all_tok_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in min_prob_all_token_aw_text]\n",
    "number_phone_all_tok_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in number_phonemes_all_token_aw_text]\n",
    "Conditional_Probability_Average_token_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in Conditional_Probability_Average_token_aw_text_no_nan]\n",
    "Ortho_N_token_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in Ortho_N_token_aw_text_no_nan]\n",
    "Phono_N_token_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in Phono_N_token_aw_text_no_nan]\n",
    "Phono_N_H_token_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in Phono_N_H_token_aw_text_no_nan]\n",
    "OG_N_token_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in OG_N_token_aw_text_no_nan]\n",
    "OG_N_H_token_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in OG_N_H_token_aw_text_no_nan]\n",
    "Freq_N_token_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in Freq_N_token_aw_text_no_nan]\n",
    "Freq_N_P_token_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in Freq_N_P_token_aw_text_no_nan]\n",
    "Freq_N_PH_token_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in Freq_N_PH_token_aw_text_no_nan]\n",
    "Freq_N_OG_token_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in Freq_N_OG_token_aw_text_no_nan]\n",
    "Freq_N_OGH_token_aw = [sum(sub_list) / len(sub_list)  if sub_list else 0for sub_list in Freq_N_OGH_token_aw_text_no_nan]\n",
    "OLD_token_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in OLD_token_aw_text_no_nan]\n",
    "OLDF_token_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in OLDF_token_aw_text_no_nan]\n",
    "PLD_token_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in PLD_token_aw_text_no_nan]\n",
    "PLDF_token_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in PLDF_token_aw_text_no_nan]\n",
    "subtlexus_log_freq_token_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in subtlexus_log_freq_token_aw_text_no_nan]\n",
    "subtlexus_log_cd_token_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in subtlexus_log_cd_token_aw_text_no_nan]\n",
    "coca_maga_cd_token_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in coca_maga_cd_token_aw_text_no_nan]\n",
    "coca_mag_log_freq_token_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in coca_mag_log_freq_token_aw_text_no_nan]\n",
    "num_rhymes_full_elp_token_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_rhymes_full_elp_token_aw_text_no_nan]\n",
    "num_rhymes_1000_coca_token_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_rhymes_1000_coca_token_aw_text_no_nan]\n",
    "num_rhymes_2500_coca_token_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_rhymes_2500_coca_token_aw_text_no_nan]\n",
    "num_rhymes_5000_coca_token_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_rhymes_5000_coca_token_aw_text_no_nan]\n",
    "num_rhymes_10000_coca_token_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_rhymes_10000_coca_token_aw_text_no_nan]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "231b7c54-15da-43c6-8dd5-be94f193ea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add to database\n",
    "\n",
    "df2 = df.assign(nw_token = nw_token_text, \n",
    "                num_syl_tok_aw = num_syl_tok_aw, \n",
    "                num_let_tok_aw = num_let_tok_aw, \n",
    "                num_phone_tok_aw = num_phone_tok_aw, \n",
    "                discrepancy_raw_tok_aw = discrepancy_raw_tok_aw, \n",
    "                discrepancy_ratio_tok_aw = discrepancy_ratio_tok_aw, \n",
    "                avg_syl_length_tok_aw = avg_syl_length_tok_aw, \n",
    "                num_cons_char_tok_aw = num_cons_char_tok_aw, \n",
    "                num_vowel_char_tok_aw = num_vowel_char_tok_aw, \n",
    "                num_cons_phone_tok_aw = num_cons_phone_tok_aw, \n",
    "                num_vowel_phone_tok_aw = num_vowel_phone_tok_aw, \n",
    "                avg_phone_per_char_cons_tok_aw = avg_phone_per_char_cons_tok_aw, \n",
    "                avg_phone_per_char_vowel_tok_aw = avg_phone_per_char_vowel_tok_aw, \n",
    "                avg_phone_per_char_all_tok_aw = avg_phone_per_char_all_tok_aw, \n",
    "                reverse_prior_prob_cons_tok_aw = prior_prob_cons_tok_aw, \n",
    "                max_prob_cons_tok_aw = max_prob_cons_tok_aw, \n",
    "                min_prob_cons_tok_aw = min_prob_cons_tok_aw, \n",
    "                mid_prob_cons_tok_aw = mid_prob_cons_tok_aw, \n",
    "                number_phone_cons_tok_aw = number_phone_cons_tok_aw, \n",
    "                reverse_prior_prob_vowel_tok_aw = prior_prob_vowel_tok_aw, \n",
    "                max_prob_vowel_tok_aw = max_prob_vowel_tok_aw, \n",
    "                min_prob_vowel_tok_aw = min_prob_vowel_tok_aw, \n",
    "                mid_prob_vowel_tok_aw = mid_prob_vowel_tok_aw, \n",
    "                number_phone_vowel_tok_aw = number_phone_vowel_tok_aw, \n",
    "                reverse_prior_prob_all_tok_aw = prior_prob_all_tok_aw, \n",
    "                max_prob_all_tok_aw = max_prob_all_tok_aw, \n",
    "                mid_prob_all_tok_aw = mid_prob_all_tok_aw, \n",
    "                min_prob_all_tok_aw = min_prob_all_tok_aw, \n",
    "                number_phone_all_tok_aw = number_phone_all_tok_aw,\n",
    "                Conditional_Probability_Average_tok_aw = Conditional_Probability_Average_token_aw,\n",
    "                Ortho_N_tok_aw = Ortho_N_token_aw,\n",
    "                Phono_N_tok_aw = Phono_N_token_aw,\n",
    "                Phono_N_H_tok_aw = Phono_N_H_token_aw,\n",
    "                OG_N_tok_aw = OG_N_token_aw,\n",
    "                OG_N_H_tok_aw = OG_N_H_token_aw,\n",
    "                Freq_N_tok_aw = Freq_N_token_aw,\n",
    "                Freq_N_P_tok_aw = Freq_N_P_token_aw,\n",
    "                Freq_N_PH_tok_aw = Freq_N_PH_token_aw,\n",
    "                Freq_N_OG_tok_aw = Freq_N_OG_token_aw,\n",
    "                Freq_N_OGH_tok_aw = Freq_N_OGH_token_aw,\n",
    "                OLD_tok_aw = OLD_token_aw,\n",
    "                OLDF_tok_aw = OLDF_token_aw,\n",
    "                PLD_tok_aw = PLD_token_aw,\n",
    "                PLDF_tok_aw = PLDF_token_aw,\n",
    "                subtlexus_log_freq_tok_aw = subtlexus_log_freq_token_aw,\n",
    "                subtlexus_log_cd_tok_aw = subtlexus_log_cd_token_aw,\n",
    "                coca_maga_cd_tok_aw = coca_maga_cd_token_aw,\n",
    "                coca_mag_log_freq_tok_aw = coca_mag_log_freq_token_aw,\n",
    "                num_rhymes_full_elp_tok_aw = num_rhymes_full_elp_token_aw,\n",
    "                num_rhymes_1000_coca_tok_aw = num_rhymes_1000_coca_token_aw,\n",
    "                num_rhymes_2500_coca_tok_aw = num_rhymes_2500_coca_token_aw,\n",
    "                num_rhymes_5000_coca_tok_aw = num_rhymes_5000_coca_token_aw,\n",
    "                num_rhymes_10000_coca_tok_aw = num_rhymes_10000_coca_token_aw)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b729cf92-9395-423c-9eda-ffa1dc4efffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#weighted scores for prob based on number of phonemes\n",
    "\n",
    "df2['weight_max_prob_cons_tok_aw'] = df2['max_prob_cons_tok_aw'] / df2['number_phone_cons_tok_aw']\n",
    "df2['weight_min_prob_cons_tok_aw'] = df2['min_prob_cons_tok_aw'] / df2['number_phone_cons_tok_aw']\n",
    "df2['weight_mid_prob_cons_tok_aw'] = df2['mid_prob_cons_tok_aw'] / df2['number_phone_cons_tok_aw']\n",
    "\n",
    "df2['weight_max_prob_vowel_tok_aw'] = df2['max_prob_vowel_tok_aw'] / df2['number_phone_vowel_tok_aw']\n",
    "df2['weight_min_prob_vowel_tok_aw'] = df2['min_prob_vowel_tok_aw'] / df2['number_phone_vowel_tok_aw']\n",
    "df2['weight_mid_prob_vowel_tok_aw'] = df2['mid_prob_vowel_tok_aw'] / df2['number_phone_vowel_tok_aw']\n",
    "\n",
    "df2['weight_max_prob_all_tok_aw'] = df2['max_prob_all_tok_aw'] / df2['number_phone_all_tok_aw']\n",
    "df2['weight_mid_prob_all_tok_aw'] = df2['mid_prob_all_tok_aw'] / df2['number_phone_all_tok_aw']\n",
    "df2['weight_min_prob_all_tok_aw'] = df2['min_prob_all_tok_aw'] / df2['number_phone_all_tok_aw']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7e520fdd-64e4-41ff-82b5-b43bb6190263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 90)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3a91c208-aa14-45b2-bac3-43c8586bb67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4 entries, 0 to 3\n",
      "Data columns (total 90 columns):\n",
      " #   Column                                  Non-Null Count  Dtype  \n",
      "---  ------                                  --------------  -----  \n",
      " 0   ID                                      4 non-null      int64  \n",
      " 1   Author                                  4 non-null      object \n",
      " 2   Title                                   4 non-null      object \n",
      " 3   Anthology                               0 non-null      float64\n",
      " 4   URL                                     4 non-null      object \n",
      " 5   Pub Year                                4 non-null      int64  \n",
      " 6   Categ                                   4 non-null      object \n",
      " 7   Sub Cat                                 0 non-null      float64\n",
      " 8   Lexile Band                             4 non-null      int64  \n",
      " 9   Location                                4 non-null      object \n",
      " 10  License                                 0 non-null      float64\n",
      " 11  MPAA Max                                4 non-null      object \n",
      " 12  MPAA #Max                               4 non-null      int64  \n",
      " 13  MPAA# Avg                               4 non-null      float64\n",
      " 14  Excerpt                                 4 non-null      object \n",
      " 15  Google WC                               4 non-null      int64  \n",
      " 16  Sentence Count                          4 non-null      int64  \n",
      " 17  Paragraphs                              4 non-null      int64  \n",
      " 18  BT_easiness                             4 non-null      float64\n",
      " 19  s.e.                                    4 non-null      float64\n",
      " 20  Flesch-Reading-Ease                     4 non-null      float64\n",
      " 21  Flesch-Kincaid-Grade-Level              4 non-null      float64\n",
      " 22  Automated Readability Index             4 non-null      float64\n",
      " 23  SMOG Readability                        4 non-null      int64  \n",
      " 24  New Dale-Chall Readability Formula      4 non-null      float64\n",
      " 25  CAREC                                   4 non-null      float64\n",
      " 26  CAREC_M                                 4 non-null      float64\n",
      " 27  CML2RI                                  4 non-null      float64\n",
      " 28  nw_token                                4 non-null      int64  \n",
      " 29  num_syl_tok_aw                          4 non-null      float64\n",
      " 30  num_let_tok_aw                          4 non-null      float64\n",
      " 31  num_phone_tok_aw                        4 non-null      float64\n",
      " 32  discrepancy_raw_tok_aw                  4 non-null      float64\n",
      " 33  discrepancy_ratio_tok_aw                4 non-null      float64\n",
      " 34  avg_syl_length_tok_aw                   4 non-null      float64\n",
      " 35  num_cons_char_tok_aw                    4 non-null      float64\n",
      " 36  num_vowel_char_tok_aw                   4 non-null      float64\n",
      " 37  num_cons_phone_tok_aw                   4 non-null      float64\n",
      " 38  num_vowel_phone_tok_aw                  4 non-null      float64\n",
      " 39  avg_phone_per_char_cons_tok_aw          4 non-null      float64\n",
      " 40  avg_phone_per_char_vowel_tok_aw         4 non-null      float64\n",
      " 41  avg_phone_per_char_all_tok_aw           4 non-null      float64\n",
      " 42  reverse_prior_prob_cons_tok_aw          4 non-null      float64\n",
      " 43  max_prob_cons_tok_aw                    4 non-null      float64\n",
      " 44  min_prob_cons_tok_aw                    4 non-null      float64\n",
      " 45  mid_prob_cons_tok_aw                    4 non-null      float64\n",
      " 46  number_phone_cons_tok_aw                4 non-null      float64\n",
      " 47  reverse_prior_prob_vowel_tok_aw         4 non-null      float64\n",
      " 48  max_prob_vowel_tok_aw                   4 non-null      float64\n",
      " 49  min_prob_vowel_tok_aw                   4 non-null      float64\n",
      " 50  mid_prob_vowel_tok_aw                   4 non-null      float64\n",
      " 51  number_phone_vowel_tok_aw               4 non-null      float64\n",
      " 52  reverse_prior_prob_all_tok_aw           4 non-null      float64\n",
      " 53  max_prob_all_tok_aw                     4 non-null      float64\n",
      " 54  mid_prob_all_tok_aw                     4 non-null      float64\n",
      " 55  min_prob_all_tok_aw                     4 non-null      float64\n",
      " 56  number_phone_all_tok_aw                 4 non-null      float64\n",
      " 57  Conditional_Probability_Average_tok_aw  4 non-null      float64\n",
      " 58  Ortho_N_tok_aw                          4 non-null      float64\n",
      " 59  Phono_N_tok_aw                          4 non-null      float64\n",
      " 60  Phono_N_H_tok_aw                        4 non-null      float64\n",
      " 61  OG_N_tok_aw                             4 non-null      float64\n",
      " 62  OG_N_H_tok_aw                           4 non-null      float64\n",
      " 63  Freq_N_tok_aw                           4 non-null      float64\n",
      " 64  Freq_N_P_tok_aw                         4 non-null      float64\n",
      " 65  Freq_N_PH_tok_aw                        4 non-null      float64\n",
      " 66  Freq_N_OG_tok_aw                        4 non-null      float64\n",
      " 67  Freq_N_OGH_tok_aw                       4 non-null      float64\n",
      " 68  OLD_tok_aw                              4 non-null      float64\n",
      " 69  OLDF_tok_aw                             4 non-null      float64\n",
      " 70  PLD_tok_aw                              4 non-null      float64\n",
      " 71  PLDF_tok_aw                             4 non-null      float64\n",
      " 72  subtlexus_log_freq_tok_aw               4 non-null      float64\n",
      " 73  subtlexus_log_cd_tok_aw                 4 non-null      float64\n",
      " 74  coca_maga_cd_tok_aw                     4 non-null      float64\n",
      " 75  coca_mag_log_freq_tok_aw                4 non-null      float64\n",
      " 76  num_rhymes_full_elp_tok_aw              4 non-null      float64\n",
      " 77  num_rhymes_1000_coca_tok_aw             4 non-null      float64\n",
      " 78  num_rhymes_2500_coca_tok_aw             4 non-null      float64\n",
      " 79  num_rhymes_5000_coca_tok_aw             4 non-null      float64\n",
      " 80  num_rhymes_10000_coca_tok_aw            4 non-null      float64\n",
      " 81  weight_max_prob_cons_tok_aw             4 non-null      float64\n",
      " 82  weight_min_prob_cons_tok_aw             4 non-null      float64\n",
      " 83  weight_mid_prob_cons_tok_aw             4 non-null      float64\n",
      " 84  weight_max_prob_vowel_tok_aw            4 non-null      float64\n",
      " 85  weight_min_prob_vowel_tok_aw            4 non-null      float64\n",
      " 86  weight_mid_prob_vowel_tok_aw            4 non-null      float64\n",
      " 87  weight_max_prob_all_tok_aw              4 non-null      float64\n",
      " 88  weight_mid_prob_all_tok_aw              4 non-null      float64\n",
      " 89  weight_min_prob_all_tok_aw              4 non-null      float64\n",
      "dtypes: float64(74), int64(9), object(7)\n",
      "memory usage: 2.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df2_info = df2.info(max_cols=len(df2.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d49d4cfd-c46d-4f76-a55d-c0113b0ca33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Author</th>\n",
       "      <th>Title</th>\n",
       "      <th>Anthology</th>\n",
       "      <th>URL</th>\n",
       "      <th>Pub Year</th>\n",
       "      <th>Categ</th>\n",
       "      <th>Sub Cat</th>\n",
       "      <th>Lexile Band</th>\n",
       "      <th>Location</th>\n",
       "      <th>...</th>\n",
       "      <th>num_rhymes_10000_coca_tok_aw</th>\n",
       "      <th>weight_max_prob_cons_tok_aw</th>\n",
       "      <th>weight_min_prob_cons_tok_aw</th>\n",
       "      <th>weight_mid_prob_cons_tok_aw</th>\n",
       "      <th>weight_max_prob_vowel_tok_aw</th>\n",
       "      <th>weight_min_prob_vowel_tok_aw</th>\n",
       "      <th>weight_mid_prob_vowel_tok_aw</th>\n",
       "      <th>weight_max_prob_all_tok_aw</th>\n",
       "      <th>weight_mid_prob_all_tok_aw</th>\n",
       "      <th>weight_min_prob_all_tok_aw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>400</td>\n",
       "      <td>Carolyn Wells</td>\n",
       "      <td>Patty's Suitors</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.gutenberg.org/cache/epub/5631/pg563...</td>\n",
       "      <td>1914</td>\n",
       "      <td>Lit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>900</td>\n",
       "      <td>mid</td>\n",
       "      <td>...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.349227</td>\n",
       "      <td>0.132859</td>\n",
       "      <td>0.243616</td>\n",
       "      <td>0.091409</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.045934</td>\n",
       "      <td>0.155603</td>\n",
       "      <td>0.095155</td>\n",
       "      <td>0.033426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>401</td>\n",
       "      <td>Carolyn Wells</td>\n",
       "      <td>Two Little Women on a Holiday</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.gutenberg.org/cache/epub/5893/pg589...</td>\n",
       "      <td>1917</td>\n",
       "      <td>Lit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>700</td>\n",
       "      <td>mid</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.422618</td>\n",
       "      <td>0.165176</td>\n",
       "      <td>0.291029</td>\n",
       "      <td>0.071109</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.035675</td>\n",
       "      <td>0.151319</td>\n",
       "      <td>0.093944</td>\n",
       "      <td>0.037878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>402</td>\n",
       "      <td>Carolyn Wells</td>\n",
       "      <td>Patty Blossom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.gutenberg.org/cache/epub/20945/pg20...</td>\n",
       "      <td>1917</td>\n",
       "      <td>Lit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>900</td>\n",
       "      <td>mid</td>\n",
       "      <td>...</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.501312</td>\n",
       "      <td>0.236655</td>\n",
       "      <td>0.369590</td>\n",
       "      <td>0.076395</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.038335</td>\n",
       "      <td>0.146623</td>\n",
       "      <td>0.093084</td>\n",
       "      <td>0.039343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>403</td>\n",
       "      <td>CHARLES KINGSLEY</td>\n",
       "      <td>THE WATER-BABIES\\nA Fairy Tale for a Land-Baby</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.gutenberg.org/files/25564/25564-h/2...</td>\n",
       "      <td>1863</td>\n",
       "      <td>Lit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1300</td>\n",
       "      <td>mid</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.594600</td>\n",
       "      <td>0.400600</td>\n",
       "      <td>0.489800</td>\n",
       "      <td>0.092550</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>0.046527</td>\n",
       "      <td>0.192960</td>\n",
       "      <td>0.135182</td>\n",
       "      <td>0.080524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID            Author                                           Title  \\\n",
       "0  400     Carolyn Wells                                 Patty's Suitors   \n",
       "1  401     Carolyn Wells                   Two Little Women on a Holiday   \n",
       "2  402     Carolyn Wells                                   Patty Blossom   \n",
       "3  403  CHARLES KINGSLEY  THE WATER-BABIES\\nA Fairy Tale for a Land-Baby   \n",
       "\n",
       "   Anthology                                                URL  Pub Year  \\\n",
       "0        NaN  http://www.gutenberg.org/cache/epub/5631/pg563...      1914   \n",
       "1        NaN  http://www.gutenberg.org/cache/epub/5893/pg589...      1917   \n",
       "2        NaN  http://www.gutenberg.org/cache/epub/20945/pg20...      1917   \n",
       "3        NaN  http://www.gutenberg.org/files/25564/25564-h/2...      1863   \n",
       "\n",
       "  Categ  Sub Cat  Lexile Band Location  ...  num_rhymes_10000_coca_tok_aw  \\\n",
       "0   Lit      NaN          900      mid  ...                      0.166667   \n",
       "1   Lit      NaN          700      mid  ...                      0.000000   \n",
       "2   Lit      NaN          900      mid  ...                      2.500000   \n",
       "3   Lit      NaN         1300      mid  ...                      0.000000   \n",
       "\n",
       "  weight_max_prob_cons_tok_aw  weight_min_prob_cons_tok_aw  \\\n",
       "0                    0.349227                     0.132859   \n",
       "1                    0.422618                     0.165176   \n",
       "2                    0.501312                     0.236655   \n",
       "3                    0.594600                     0.400600   \n",
       "\n",
       "   weight_mid_prob_cons_tok_aw weight_max_prob_vowel_tok_aw  \\\n",
       "0                     0.243616                     0.091409   \n",
       "1                     0.291029                     0.071109   \n",
       "2                     0.369590                     0.076395   \n",
       "3                     0.489800                     0.092550   \n",
       "\n",
       "   weight_min_prob_vowel_tok_aw  weight_mid_prob_vowel_tok_aw  \\\n",
       "0                      0.000459                      0.045934   \n",
       "1                      0.000241                      0.035675   \n",
       "2                      0.000276                      0.038335   \n",
       "3                      0.000505                      0.046527   \n",
       "\n",
       "   weight_max_prob_all_tok_aw  weight_mid_prob_all_tok_aw  \\\n",
       "0                    0.155603                    0.095155   \n",
       "1                    0.151319                    0.093944   \n",
       "2                    0.146623                    0.093084   \n",
       "3                    0.192960                    0.135182   \n",
       "\n",
       "   weight_min_prob_all_tok_aw  \n",
       "0                    0.033426  \n",
       "1                    0.037878  \n",
       "2                    0.039343  \n",
       "3                    0.080524  \n",
       "\n",
       "[4 rows x 90 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#what's in it\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a158fc00-a8f8-4868-a8fb-130fd13f0554",
   "metadata": {},
   "source": [
    "**THIS IS THE MAIN CODE FOR UNLEMMATIZED DATA**\n",
    "\n",
    "        - Content words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c375a4c0-ba92-47cd-b5b5-8e32d6f40083",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_syllables_token_cw_text = []\n",
    "num_letters_token_cw_text = []\n",
    "num_phonemes_token_cw_text = []\n",
    "discrepancy_raw_token_cw_text = []\n",
    "discrepancy_ratio_token_cw_text = []\n",
    "avg_syllable_length_token_cw_text = []\n",
    "num_consonants_characters_token_cw_text = []\n",
    "num_vowel_characters_token_cw_text = []\n",
    "num_consonants_phonemes_token_cw_text = []\n",
    "num_vowel_phonemes_token_cw_text = []\n",
    "avg_phonemes_per_character_consonants_token_cw_text = []\n",
    "avg_phonemes_per_character_vowels_token_cw_text = []\n",
    "avg_phonemes_per_character_all_token_cw_text = []\n",
    "prior_prob_cons_token_cw_text = []\n",
    "max_prob_cons_token_cw_text = []\n",
    "min_prob_cons_token_cw_text = []\n",
    "mid_prob_cons_token_cw_text = []\n",
    "number_phonemes_cons_token_cw_text = []\n",
    "prior_prob_vowel_token_cw_text = []\n",
    "max_prob_vowel_token_cw_text = []\n",
    "min_prob_vowel_token_cw_text = []\n",
    "mid_prob_vowel_token_cw_text = []\n",
    "number_phonemes_vowel_token_cw_text = []\n",
    "prior_prob_all_token_cw_text = []\n",
    "max_prob_all_token_cw_text = []\n",
    "mid_prob_all_token_cw_text = []\n",
    "min_prob_all_token_cw_text = []\n",
    "number_phonemes_all_token_cw_text = []\n",
    "Conditional_Probability_Average_token_cw_text = []\n",
    "Ortho_N_token_cw_text = []\n",
    "Phono_N_token_cw_text = []\n",
    "Phono_N_H_token_cw_text = []\n",
    "OG_N_token_cw_text = []\n",
    "OG_N_H_token_cw_text = []\n",
    "Freq_N_token_cw_text = []\n",
    "Freq_N_P_token_cw_text = []\n",
    "Freq_N_PH_token_cw_text = []\n",
    "Freq_N_OG_token_cw_text = []\n",
    "Freq_N_OGH_token_cw_text = []\n",
    "OLD_token_cw_text = []\n",
    "OLDF_token_cw_text = []\n",
    "PLD_token_cw_text = []\n",
    "PLDF_token_cw_text = []\n",
    "subtlexus_log_freq_token_cw_text = []\n",
    "subtlexus_log_cd_token_cw_text = []\n",
    "coca_maga_cd_token_cw_text = []\n",
    "coca_mag_log_freq_token_cw_text = []\n",
    "num_rhymes_full_elp_token_cw_text = []\n",
    "num_rhymes_1000_coca_token_cw_text = []\n",
    "num_rhymes_2500_coca_token_cw_text = []\n",
    "num_rhymes_5000_coca_token_cw_text = []\n",
    "num_rhymes_10000_coca_token_cw_text = []\n",
    "\n",
    "\n",
    "for tokenized_doc in df_docs:\n",
    "    num_syllables_token_cw = []\n",
    "    num_letters_token_cw = []\n",
    "    num_phonemes_token_cw = []\n",
    "    discrepancy_raw_token_cw = []\n",
    "    discrepancy_ratio_token_cw = []\n",
    "    avg_syllable_length_token_cw = []\n",
    "    num_consonants_characters_token_cw = []\n",
    "    num_vowel_characters_token_cw = []\n",
    "    num_consonants_phonemes_token_cw = []\n",
    "    num_vowel_phonemes_token_cw = []\n",
    "    avg_phonemes_per_character_consonants_token_cw = []\n",
    "    avg_phonemes_per_character_vowels_token_cw = []\n",
    "    avg_phonemes_per_character_all_token_cw = []\n",
    "    prior_prob_cons_token_cw = []\n",
    "    max_prob_cons_token_cw = []\n",
    "    min_prob_cons_token_cw = []\n",
    "    mid_prob_cons_token_cw = []\n",
    "    number_phonemes_cons_token_cw = []\n",
    "    prior_prob_vowel_token_cw = []\n",
    "    max_prob_vowel_token_cw = []\n",
    "    min_prob_vowel_token_cw = []\n",
    "    mid_prob_vowel_token_cw = []\n",
    "    number_phonemes_vowel_token_cw = []\n",
    "    prior_prob_all_token_cw = []\n",
    "    max_prob_all_token_cw = []\n",
    "    mid_prob_all_token_cw = []\n",
    "    min_prob_all_token_cw = []\n",
    "    number_phonemes_all_token_cw = []\n",
    "    Conditional_Probability_Average_token_cw = []\n",
    "    Ortho_N_token_cw = []\n",
    "    Phono_N_token_cw = []\n",
    "    Phono_N_H_token_cw = []\n",
    "    OG_N_token_cw = []\n",
    "    OG_N_H_token_cw = []\n",
    "    Freq_N_token_cw = []\n",
    "    Freq_N_P_token_cw = []\n",
    "    Freq_N_PH_token_cw = []\n",
    "    Freq_N_OG_token_cw = []\n",
    "    Freq_N_OGH_token_cw = []\n",
    "    OLD_token_cw = []\n",
    "    OLDF_token_cw = []\n",
    "    PLD_token_cw = []\n",
    "    PLDF_token_cw = []\n",
    "    subtlexus_log_freq_token_cw = []\n",
    "    subtlexus_log_cd_token_cw = []\n",
    "    coca_maga_cd_token_cw = []\n",
    "    coca_mag_log_freq_token_cw = []\n",
    "    num_rhymes_full_elp_token_cw = []\n",
    "    num_rhymes_1000_coca_token_cw = []\n",
    "    num_rhymes_2500_coca_token_cw = []\n",
    "    num_rhymes_5000_coca_token_cw = []\n",
    "    num_rhymes_10000_coca_token_cw = []\n",
    "    for token in tokenized_doc:\n",
    "        if not token.is_stop and not token.is_punct:\n",
    "            #print(token.text)\n",
    "            try:\n",
    "                val = decoding_dic[token.text]\n",
    "                #print(key)\n",
    "                #print(val[0])\n",
    "                num_syllables_token_cw.append(val[0])\n",
    "                num_letters_token_cw.append(val[1])\n",
    "                num_phonemes_token_cw.append(val[2])\n",
    "                discrepancy_raw_token_cw.append(val[3])\n",
    "                discrepancy_ratio_token_cw.append(val[4])\n",
    "                avg_syllable_length_token_cw.append(val[5])\n",
    "                num_consonants_characters_token_cw.append(val[6])\n",
    "                num_vowel_characters_token_cw.append(val[7])\n",
    "                num_consonants_phonemes_token_cw.append(val[8])\n",
    "                num_vowel_phonemes_token_cw.append(val[9])\n",
    "                avg_phonemes_per_character_consonants_token_cw.append(val[10])\n",
    "                avg_phonemes_per_character_vowels_token_cw.append(val[11])\n",
    "                avg_phonemes_per_character_all_token_cw.append(val[12])\n",
    "                prior_prob_cons_token_cw.append(val[13])\n",
    "                max_prob_cons_token_cw.append(val[14])\n",
    "                min_prob_cons_token_cw.append(val[15])\n",
    "                mid_prob_cons_token_cw.append(val[16])\n",
    "                number_phonemes_cons_token_cw.append(val[17])\n",
    "                prior_prob_vowel_token_cw.append(val[18])\n",
    "                max_prob_vowel_token_cw.append(val[19])\n",
    "                min_prob_vowel_token_cw.append(val[20])\n",
    "                mid_prob_vowel_token_cw.append(val[21])\n",
    "                number_phonemes_vowel_token_cw.append(val[22])\n",
    "                prior_prob_all_token_cw.append(val[23])\n",
    "                max_prob_all_token_cw.append(val[24])\n",
    "                mid_prob_all_token_cw.append(val[25])\n",
    "                min_prob_all_token_cw.append(val[26])\n",
    "                number_phonemes_all_token_cw.append(val[27])\n",
    "                Conditional_Probability_Average_token_cw.append(val[28])\n",
    "                Ortho_N_token_cw.append(val[29])\n",
    "                Phono_N_token_cw.append(val[30])\n",
    "                Phono_N_H_token_cw.append(val[31])\n",
    "                OG_N_token_cw.append(val[32])\n",
    "                OG_N_H_token_cw.append(val[33])\n",
    "                Freq_N_token_cw.append(val[34])\n",
    "                Freq_N_P_token_cw.append(val[35])\n",
    "                Freq_N_PH_token_cw.append(val[36])\n",
    "                Freq_N_OG_token_cw.append(val[37])\n",
    "                Freq_N_OGH_token_cw.append(val[38])\n",
    "                OLD_token_cw.append(val[39])\n",
    "                OLDF_token_cw.append(val[40])\n",
    "                PLD_token_cw.append(val[41])\n",
    "                PLDF_token_cw.append(val[42])\n",
    "                subtlexus_log_freq_token_cw.append(val[43])\n",
    "                subtlexus_log_cd_token_cw.append(val[44])\n",
    "                coca_maga_cd_token_cw.append(val[45])\n",
    "                coca_mag_log_freq_token_cw.append(val[46])\n",
    "                num_rhymes_full_elp_token_cw.append(val[47])\n",
    "                num_rhymes_1000_coca_token_cw.append(val[48])\n",
    "                num_rhymes_2500_coca_token_cw.append(val[49])\n",
    "                num_rhymes_5000_coca_token_cw.append(val[50])\n",
    "                num_rhymes_10000_coca_token_cw.append(val[51])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    num_syllables_token_cw_text.append(num_syllables_token_cw)\n",
    "    num_letters_token_cw_text.append(num_letters_token_cw)\n",
    "    num_phonemes_token_cw_text.append(num_phonemes_token_cw)\n",
    "    discrepancy_raw_token_cw_text.append(discrepancy_raw_token_cw)\n",
    "    discrepancy_ratio_token_cw_text.append(discrepancy_ratio_token_cw)\n",
    "    avg_syllable_length_token_cw_text.append(avg_syllable_length_token_cw)\n",
    "    num_consonants_characters_token_cw_text.append(num_consonants_characters_token_cw)\n",
    "    num_vowel_characters_token_cw_text.append(num_vowel_characters_token_cw)\n",
    "    num_consonants_phonemes_token_cw_text.append(num_consonants_phonemes_token_cw)\n",
    "    num_vowel_phonemes_token_cw_text.append(num_vowel_phonemes_token_cw)\n",
    "    avg_phonemes_per_character_consonants_token_cw_text.append(avg_phonemes_per_character_consonants_token_cw)\n",
    "    avg_phonemes_per_character_vowels_token_cw_text.append(avg_phonemes_per_character_vowels_token_cw)\n",
    "    avg_phonemes_per_character_all_token_cw_text.append(avg_phonemes_per_character_all_token_cw)\n",
    "    prior_prob_cons_token_cw_text.append(prior_prob_cons_token_cw)\n",
    "    max_prob_cons_token_cw_text.append(max_prob_cons_token_cw)\n",
    "    min_prob_cons_token_cw_text.append(min_prob_cons_token_cw)\n",
    "    mid_prob_cons_token_cw_text.append(mid_prob_cons_token_cw)\n",
    "    number_phonemes_cons_token_cw_text.append(number_phonemes_cons_token_cw)\n",
    "    prior_prob_vowel_token_cw_text.append(prior_prob_vowel_token_cw)\n",
    "    max_prob_vowel_token_cw_text.append(max_prob_vowel_token_cw)\n",
    "    min_prob_vowel_token_cw_text.append(min_prob_vowel_token_cw)\n",
    "    mid_prob_vowel_token_cw_text.append(mid_prob_vowel_token_cw)\n",
    "    number_phonemes_vowel_token_cw_text.append(number_phonemes_vowel_token_cw)\n",
    "    prior_prob_all_token_cw_text.append(prior_prob_all_token_cw)\n",
    "    max_prob_all_token_cw_text.append(max_prob_all_token_cw)\n",
    "    mid_prob_all_token_cw_text.append(mid_prob_all_token_cw)\n",
    "    min_prob_all_token_cw_text.append(min_prob_all_token_cw)\n",
    "    number_phonemes_all_token_cw_text.append(number_phonemes_all_token_cw)\n",
    "    Conditional_Probability_Average_token_cw_text.append(Conditional_Probability_Average_token_cw)\n",
    "    Ortho_N_token_cw_text.append(Ortho_N_token_cw)\n",
    "    Phono_N_token_cw_text.append(Phono_N_token_cw)\n",
    "    Phono_N_H_token_cw_text.append(Phono_N_H_token_cw)\n",
    "    OG_N_token_cw_text.append(OG_N_token_cw)\n",
    "    OG_N_H_token_cw_text.append(OG_N_H_token_cw)\n",
    "    Freq_N_token_cw_text.append(Freq_N_token_cw)\n",
    "    Freq_N_P_token_cw_text.append(Freq_N_P_token_cw)\n",
    "    Freq_N_PH_token_cw_text.append(Freq_N_PH_token_cw)\n",
    "    Freq_N_OG_token_cw_text.append(Freq_N_OG_token_cw)\n",
    "    Freq_N_OGH_token_cw_text.append(Freq_N_OGH_token_cw)\n",
    "    OLD_token_cw_text.append(OLD_token_cw)\n",
    "    OLDF_token_cw_text.append(OLDF_token_cw)\n",
    "    PLD_token_cw_text.append(PLD_token_cw)\n",
    "    PLDF_token_cw_text.append(PLDF_token_cw)\n",
    "    subtlexus_log_freq_token_cw_text.append(subtlexus_log_freq_token_cw)\n",
    "    subtlexus_log_cd_token_cw_text.append(subtlexus_log_cd_token_cw)\n",
    "    coca_maga_cd_token_cw_text.append(coca_maga_cd_token_cw)\n",
    "    coca_mag_log_freq_token_cw_text.append(coca_mag_log_freq_token_cw)\n",
    "    num_rhymes_full_elp_token_cw_text.append(num_rhymes_full_elp_token_cw)\n",
    "    num_rhymes_1000_coca_token_cw_text.append(num_rhymes_1000_coca_token_cw)\n",
    "    num_rhymes_2500_coca_token_cw_text.append(num_rhymes_2500_coca_token_cw)\n",
    "    num_rhymes_5000_coca_token_cw_text.append(num_rhymes_5000_coca_token_cw)\n",
    "    num_rhymes_10000_coca_token_cw_text.append(num_rhymes_10000_coca_token_cw)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e3305b7d-9a6b-48de-8dd2-45714bbbe0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove nan's from the list of lists\n",
    "\n",
    "Conditional_Probability_Average_token_cw_text_no_nan = remove_nan(Conditional_Probability_Average_token_cw_text)\n",
    "Ortho_N_token_cw_text_no_nan = remove_nan(Ortho_N_token_cw_text)\n",
    "Phono_N_token_cw_text_no_nan = remove_nan(Phono_N_token_cw_text)\n",
    "Phono_N_H_token_cw_text_no_nan = remove_nan(Phono_N_H_token_cw_text)\n",
    "OG_N_token_cw_text_no_nan = remove_nan(OG_N_token_cw_text)\n",
    "OG_N_H_token_cw_text_no_nan = remove_nan(OG_N_H_token_cw_text)\n",
    "Freq_N_token_cw_text_no_nan = remove_nan(Freq_N_token_cw_text)\n",
    "Freq_N_P_token_cw_text_no_nan = remove_nan(Freq_N_P_token_cw_text)\n",
    "Freq_N_PH_token_cw_text_no_nan = remove_nan(Freq_N_PH_token_cw_text)\n",
    "Freq_N_OG_token_cw_text_no_nan = remove_nan(Freq_N_OG_token_cw_text)\n",
    "Freq_N_OGH_token_cw_text_no_nan = remove_nan(Freq_N_OGH_token_cw_text)\n",
    "OLD_token_cw_text_no_nan = remove_nan(OLD_token_cw_text)\n",
    "OLDF_token_cw_text_no_nan = remove_nan(OLDF_token_cw_text)\n",
    "PLD_token_cw_text_no_nan = remove_nan(PLD_token_cw_text)\n",
    "PLDF_token_cw_text_no_nan = remove_nan(PLDF_token_cw_text)\n",
    "subtlexus_log_freq_token_cw_text_no_nan = remove_nan(subtlexus_log_freq_token_cw_text)\n",
    "subtlexus_log_cd_token_cw_text_no_nan = remove_nan(subtlexus_log_cd_token_cw_text)\n",
    "coca_maga_cd_token_cw_text_no_nan = remove_nan(coca_maga_cd_token_cw_text)\n",
    "coca_mag_log_freq_token_cw_text_no_nan = remove_nan(coca_mag_log_freq_token_cw_text)\n",
    "num_rhymes_full_elp_token_cw_text_no_nan = remove_nan(num_rhymes_full_elp_token_cw_text)\n",
    "num_rhymes_1000_coca_token_cw_text_no_nan = remove_nan(num_rhymes_1000_coca_token_cw_text)\n",
    "num_rhymes_2500_coca_token_cw_text_no_nan = remove_nan(num_rhymes_2500_coca_token_cw_text)\n",
    "num_rhymes_5000_coca_token_cw_text_no_nan = remove_nan(num_rhymes_5000_coca_token_cw_text)\n",
    "num_rhymes_10000_coca_token_cw_text_no_nan = remove_nan(num_rhymes_10000_coca_token_cw_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0b929864-7e45-4ebd-af2b-f3e1543228d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#get lists that are average of sublists\n",
    "\n",
    "num_syl_tok_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_syllables_token_cw_text] #if it is a sublist, get average, else (if it is not a sublist, empty list, return 0) \n",
    "num_let_tok_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_letters_token_cw_text]\n",
    "num_phone_tok_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_phonemes_token_cw_text]\n",
    "discrepancy_raw_tok_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in discrepancy_raw_token_cw_text]\n",
    "discrepancy_ratio_tok_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in discrepancy_ratio_token_cw_text]\n",
    "avg_syl_length_tok_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in avg_syllable_length_token_cw_text]\n",
    "num_cons_char_tok_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_consonants_characters_token_cw_text]\n",
    "num_vowel_char_tok_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_vowel_characters_token_cw_text]\n",
    "num_cons_phone_tok_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_consonants_phonemes_token_cw_text]\n",
    "num_vowel_phone_tok_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_vowel_phonemes_token_cw_text]\n",
    "avg_phone_per_char_cons_tok_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in avg_phonemes_per_character_consonants_token_cw_text]\n",
    "avg_phone_per_char_vowel_tok_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in avg_phonemes_per_character_vowels_token_cw_text]\n",
    "avg_phone_per_char_all_tok_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in avg_phonemes_per_character_all_token_cw_text]\n",
    "prior_prob_cons_tok_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in prior_prob_cons_token_cw_text]\n",
    "max_prob_cons_tok_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in max_prob_cons_token_cw_text]\n",
    "min_prob_cons_tok_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in min_prob_cons_token_cw_text]\n",
    "mid_prob_cons_tok_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in mid_prob_cons_token_cw_text]\n",
    "number_phone_cons_tok_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in number_phonemes_cons_token_cw_text]\n",
    "prior_prob_vowel_tok_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in prior_prob_vowel_token_cw_text]\n",
    "max_prob_vowel_tok_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in max_prob_vowel_token_cw_text]\n",
    "min_prob_vowel_tok_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in min_prob_vowel_token_cw_text]\n",
    "mid_prob_vowel_tok_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in mid_prob_vowel_token_cw_text]\n",
    "number_phone_vowel_tok_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in number_phonemes_vowel_token_cw_text]\n",
    "prior_prob_all_tok_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in prior_prob_all_token_cw_text]\n",
    "max_prob_all_tok_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in max_prob_all_token_cw_text]\n",
    "mid_prob_all_tok_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in mid_prob_all_token_cw_text]\n",
    "min_prob_all_tok_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in min_prob_all_token_cw_text]\n",
    "number_phone_all_tok_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in number_phonemes_all_token_cw_text]\n",
    "Conditional_Probability_Average_token_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in Conditional_Probability_Average_token_cw_text_no_nan]\n",
    "Ortho_N_token_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in Ortho_N_token_cw_text_no_nan]\n",
    "Phono_N_token_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in Phono_N_token_cw_text_no_nan]\n",
    "Phono_N_H_token_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in Phono_N_H_token_cw_text_no_nan]\n",
    "OG_N_token_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in OG_N_token_cw_text_no_nan]\n",
    "OG_N_H_token_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in OG_N_H_token_cw_text_no_nan]\n",
    "Freq_N_token_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in Freq_N_token_cw_text_no_nan]\n",
    "Freq_N_P_token_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in Freq_N_P_token_cw_text_no_nan]\n",
    "Freq_N_PH_token_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in Freq_N_PH_token_cw_text_no_nan]\n",
    "Freq_N_OG_token_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in Freq_N_OG_token_cw_text_no_nan]\n",
    "Freq_N_OGH_token_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in Freq_N_OGH_token_cw_text_no_nan]\n",
    "OLD_token_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in OLD_token_cw_text_no_nan]\n",
    "OLDF_token_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in OLDF_token_cw_text_no_nan]\n",
    "PLD_token_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in PLD_token_cw_text_no_nan]\n",
    "PLDF_token_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in PLDF_token_cw_text_no_nan]\n",
    "subtlexus_log_freq_token_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in subtlexus_log_freq_token_cw_text_no_nan]\n",
    "subtlexus_log_cd_token_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in subtlexus_log_cd_token_cw_text_no_nan]\n",
    "coca_maga_cd_token_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in coca_maga_cd_token_cw_text_no_nan]\n",
    "coca_mag_log_freq_token_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in coca_mag_log_freq_token_cw_text_no_nan]\n",
    "num_rhymes_full_elp_token_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_rhymes_full_elp_token_cw_text_no_nan]\n",
    "num_rhymes_1000_coca_token_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_rhymes_1000_coca_token_cw_text_no_nan]\n",
    "num_rhymes_2500_coca_token_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_rhymes_2500_coca_token_cw_text_no_nan]\n",
    "num_rhymes_5000_coca_token_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_rhymes_5000_coca_token_cw_text_no_nan]\n",
    "num_rhymes_10000_coca_token_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_rhymes_10000_coca_token_cw_text_no_nan]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "693ed3ab-74c7-4c75-8836-3c6024964624",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add to database\n",
    "\n",
    "df2 = df2.assign(num_syl_tok_cw = num_syl_tok_cw, \n",
    "                num_let_tok_cw = num_let_tok_cw, \n",
    "                num_phone_tok_cw = num_phone_tok_cw, \n",
    "                discrepancy_raw_tok_cw = discrepancy_raw_tok_cw, \n",
    "                discrepancy_ratio_tok_cw = discrepancy_ratio_tok_cw, \n",
    "                avg_syl_length_tok_cw = avg_syl_length_tok_cw, \n",
    "                num_cons_char_tok_cw = num_cons_char_tok_cw, \n",
    "                num_vowel_char_tok_cw = num_vowel_char_tok_cw, \n",
    "                num_cons_phone_tok_cw = num_cons_phone_tok_cw, \n",
    "                num_vowel_phone_tok_cw = num_vowel_phone_tok_cw, \n",
    "                avg_phone_per_char_cons_tok_cw = avg_phone_per_char_cons_tok_cw, \n",
    "                avg_phone_per_char_vowel_tok_cw = avg_phone_per_char_vowel_tok_cw, \n",
    "                avg_phone_per_char_all_tok_cw = avg_phone_per_char_all_tok_cw, \n",
    "                reverse_prior_prob_cons_tok_cw = prior_prob_cons_tok_cw, \n",
    "                max_prob_cons_tok_cw = max_prob_cons_tok_cw, \n",
    "                min_prob_cons_tok_cw = min_prob_cons_tok_cw, \n",
    "                mid_prob_cons_tok_cw = mid_prob_cons_tok_cw, \n",
    "                number_phone_cons_tok_cw = number_phone_cons_tok_cw, \n",
    "                reverse_prior_prob_vowel_tok_cw = prior_prob_vowel_tok_cw, \n",
    "                max_prob_vowel_tok_cw = max_prob_vowel_tok_cw, \n",
    "                min_prob_vowel_tok_cw = min_prob_vowel_tok_cw, \n",
    "                mid_prob_vowel_tok_cw = mid_prob_vowel_tok_cw, \n",
    "                number_phone_vowel_tok_cw = number_phone_vowel_tok_cw, \n",
    "                reverse_prior_prob_all_tok_cw = prior_prob_all_tok_cw, \n",
    "                max_prob_all_tok_cw = max_prob_all_tok_cw, \n",
    "                mid_prob_all_tok_cw = mid_prob_all_tok_cw, \n",
    "                min_prob_all_tok_cw = min_prob_all_tok_cw, \n",
    "                number_phone_all_tok_cw = number_phone_all_tok_cw,\n",
    "                Conditional_Probability_Average_tok_cw = Conditional_Probability_Average_token_cw,\n",
    "                Ortho_N_tok_cw = Ortho_N_token_cw,\n",
    "                Phono_N_tok_cw = Phono_N_token_cw,\n",
    "                Phono_N_H_tok_cw = Phono_N_H_token_cw,\n",
    "                OG_N_tok_cw = OG_N_token_cw,\n",
    "                OG_N_H_tok_cw = OG_N_H_token_cw,\n",
    "                Freq_N_tok_cw = Freq_N_token_cw,\n",
    "                Freq_N_P_tok_cw = Freq_N_P_token_cw,\n",
    "                Freq_N_PH_tok_cw = Freq_N_PH_token_cw,\n",
    "                Freq_N_OG_tok_cw = Freq_N_OG_token_cw,\n",
    "                Freq_N_OGH_tok_cw = Freq_N_OGH_token_cw,\n",
    "                OLD_tok_cw = OLD_token_cw,\n",
    "                OLDF_tok_cw = OLDF_token_cw,\n",
    "                PLD_tok_cw = PLD_token_cw,\n",
    "                PLDF_tok_cw = PLDF_token_cw,\n",
    "                subtlexus_log_freq_tok_cw = subtlexus_log_freq_token_cw,\n",
    "                subtlexus_log_cd_tok_cw = subtlexus_log_cd_token_cw,\n",
    "                coca_maga_cd_tok_cw = coca_maga_cd_token_cw,\n",
    "                coca_mag_log_freq_tok_cw = coca_mag_log_freq_token_cw,\n",
    "                num_rhymes_full_elp_tok_cw = num_rhymes_full_elp_token_cw,\n",
    "                num_rhymes_1000_coca_tok_cw = num_rhymes_1000_coca_token_cw,\n",
    "                num_rhymes_2500_coca_tok_cw = num_rhymes_2500_coca_token_cw,\n",
    "                num_rhymes_5000_coca_tok_cw = num_rhymes_5000_coca_token_cw,\n",
    "                num_rhymes_10000_coca_tok_cw = num_rhymes_10000_coca_token_cw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7a4e3de8-40dd-4304-a4b7-b6cc9f3bedb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#weighted scores for prob based on number of phonemes\n",
    "\n",
    "df2['weight_max_prob_cons_tok_cw'] = df2['max_prob_cons_tok_cw'] / df2['number_phone_cons_tok_cw']\n",
    "df2['weight_min_prob_cons_tok_cw'] = df2['min_prob_cons_tok_cw'] / df2['number_phone_cons_tok_cw']\n",
    "df2['weight_mid_prob_cons_tok_cw'] = df2['mid_prob_cons_tok_cw'] / df2['number_phone_cons_tok_cw']\n",
    "\n",
    "df2['weight_max_prob_vowel_tok_cw'] = df2['max_prob_vowel_tok_cw'] / df2['number_phone_vowel_tok_cw']\n",
    "df2['weight_min_prob_vowel_tok_cw'] = df2['min_prob_vowel_tok_cw'] / df2['number_phone_vowel_tok_cw']\n",
    "df2['weight_mid_prob_vowel_tok_cw'] = df2['mid_prob_vowel_tok_cw'] / df2['number_phone_vowel_tok_cw']\n",
    "\n",
    "df2['weight_max_prob_all_tok_cw'] = df2['max_prob_all_tok_cw'] / df2['number_phone_all_tok_cw']\n",
    "df2['weight_mid_prob_all_tok_cw'] = df2['mid_prob_all_tok_cw'] / df2['number_phone_all_tok_cw']\n",
    "df2['weight_min_prob_all_tok_cw'] = df2['min_prob_all_tok_cw'] / df2['number_phone_all_tok_cw']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "76bbb8e0-70c6-4db8-b59b-d6dd0846b97a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 151)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "561c2f30-d9e3-44f7-9310-78d356caf2aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Author</th>\n",
       "      <th>Title</th>\n",
       "      <th>Anthology</th>\n",
       "      <th>URL</th>\n",
       "      <th>Pub Year</th>\n",
       "      <th>Categ</th>\n",
       "      <th>Sub Cat</th>\n",
       "      <th>Lexile Band</th>\n",
       "      <th>Location</th>\n",
       "      <th>...</th>\n",
       "      <th>num_rhymes_10000_coca_tok_cw</th>\n",
       "      <th>weight_max_prob_cons_tok_cw</th>\n",
       "      <th>weight_min_prob_cons_tok_cw</th>\n",
       "      <th>weight_mid_prob_cons_tok_cw</th>\n",
       "      <th>weight_max_prob_vowel_tok_cw</th>\n",
       "      <th>weight_min_prob_vowel_tok_cw</th>\n",
       "      <th>weight_mid_prob_vowel_tok_cw</th>\n",
       "      <th>weight_max_prob_all_tok_cw</th>\n",
       "      <th>weight_mid_prob_all_tok_cw</th>\n",
       "      <th>weight_min_prob_all_tok_cw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>400</td>\n",
       "      <td>Carolyn Wells</td>\n",
       "      <td>Patty's Suitors</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.gutenberg.org/cache/epub/5631/pg563...</td>\n",
       "      <td>1914</td>\n",
       "      <td>Lit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>900</td>\n",
       "      <td>mid</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.481676</td>\n",
       "      <td>0.260746</td>\n",
       "      <td>0.363972</td>\n",
       "      <td>0.076122</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.038204</td>\n",
       "      <td>0.166956</td>\n",
       "      <td>0.111168</td>\n",
       "      <td>0.058622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>401</td>\n",
       "      <td>Carolyn Wells</td>\n",
       "      <td>Two Little Women on a Holiday</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.gutenberg.org/cache/epub/5893/pg589...</td>\n",
       "      <td>1917</td>\n",
       "      <td>Lit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>700</td>\n",
       "      <td>mid</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.537667</td>\n",
       "      <td>0.336778</td>\n",
       "      <td>0.437222</td>\n",
       "      <td>0.081071</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>0.040718</td>\n",
       "      <td>0.174466</td>\n",
       "      <td>0.121821</td>\n",
       "      <td>0.069176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>402</td>\n",
       "      <td>Carolyn Wells</td>\n",
       "      <td>Patty Blossom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.gutenberg.org/cache/epub/20945/pg20...</td>\n",
       "      <td>1917</td>\n",
       "      <td>Lit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>900</td>\n",
       "      <td>mid</td>\n",
       "      <td>...</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>0.505632</td>\n",
       "      <td>0.282864</td>\n",
       "      <td>0.397237</td>\n",
       "      <td>0.074418</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.037345</td>\n",
       "      <td>0.148066</td>\n",
       "      <td>0.098812</td>\n",
       "      <td>0.048536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>403</td>\n",
       "      <td>CHARLES KINGSLEY</td>\n",
       "      <td>THE WATER-BABIES\\nA Fairy Tale for a Land-Baby</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.gutenberg.org/files/25564/25564-h/2...</td>\n",
       "      <td>1863</td>\n",
       "      <td>Lit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1300</td>\n",
       "      <td>mid</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 151 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID            Author                                           Title  \\\n",
       "0  400     Carolyn Wells                                 Patty's Suitors   \n",
       "1  401     Carolyn Wells                   Two Little Women on a Holiday   \n",
       "2  402     Carolyn Wells                                   Patty Blossom   \n",
       "3  403  CHARLES KINGSLEY  THE WATER-BABIES\\nA Fairy Tale for a Land-Baby   \n",
       "\n",
       "   Anthology                                                URL  Pub Year  \\\n",
       "0        NaN  http://www.gutenberg.org/cache/epub/5631/pg563...      1914   \n",
       "1        NaN  http://www.gutenberg.org/cache/epub/5893/pg589...      1917   \n",
       "2        NaN  http://www.gutenberg.org/cache/epub/20945/pg20...      1917   \n",
       "3        NaN  http://www.gutenberg.org/files/25564/25564-h/2...      1863   \n",
       "\n",
       "  Categ  Sub Cat  Lexile Band Location  ...  num_rhymes_10000_coca_tok_cw  \\\n",
       "0   Lit      NaN          900      mid  ...                      0.333333   \n",
       "1   Lit      NaN          700      mid  ...                      0.000000   \n",
       "2   Lit      NaN          900      mid  ...                      3.750000   \n",
       "3   Lit      NaN         1300      mid  ...                      0.000000   \n",
       "\n",
       "  weight_max_prob_cons_tok_cw  weight_min_prob_cons_tok_cw  \\\n",
       "0                    0.481676                     0.260746   \n",
       "1                    0.537667                     0.336778   \n",
       "2                    0.505632                     0.282864   \n",
       "3                         NaN                          NaN   \n",
       "\n",
       "   weight_mid_prob_cons_tok_cw weight_max_prob_vowel_tok_cw  \\\n",
       "0                     0.363972                     0.076122   \n",
       "1                     0.437222                     0.081071   \n",
       "2                     0.397237                     0.074418   \n",
       "3                          NaN                          NaN   \n",
       "\n",
       "   weight_min_prob_vowel_tok_cw  weight_mid_prob_vowel_tok_cw  \\\n",
       "0                      0.000285                      0.038204   \n",
       "1                      0.000364                      0.040718   \n",
       "2                      0.000272                      0.037345   \n",
       "3                           NaN                           NaN   \n",
       "\n",
       "   weight_max_prob_all_tok_cw  weight_mid_prob_all_tok_cw  \\\n",
       "0                    0.166956                    0.111168   \n",
       "1                    0.174466                    0.121821   \n",
       "2                    0.148066                    0.098812   \n",
       "3                         NaN                         NaN   \n",
       "\n",
       "   weight_min_prob_all_tok_cw  \n",
       "0                    0.058622  \n",
       "1                    0.069176  \n",
       "2                    0.048536  \n",
       "3                         NaN  \n",
       "\n",
       "[4 rows x 151 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#what's in it.\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd591115-12ae-433f-a905-2ba20d83b02f",
   "metadata": {},
   "source": [
    "\n",
    "**THIS IS THE MAIN CODE FOR LEMMATIZED DATA**\n",
    "\n",
    "        - All words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1c2d6c3d-b896-444a-8067-78f27e0f8263",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_syllables_lemma_aw_text = []\n",
    "num_letters_lemma_aw_text = []\n",
    "num_phonemes_lemma_aw_text = []\n",
    "discrepancy_raw_lemma_aw_text = []\n",
    "discrepancy_ratio_lemma_aw_text = []\n",
    "avg_syllable_length_lemma_aw_text = []\n",
    "num_consonants_characters_lemma_aw_text = []\n",
    "num_vowel_characters_lemma_aw_text = []\n",
    "num_consonants_phonemes_lemma_aw_text = []\n",
    "num_vowel_phonemes_lemma_aw_text = []\n",
    "avg_phonemes_per_character_consonants_lemma_aw_text = []\n",
    "avg_phonemes_per_character_vowels_lemma_aw_text = []\n",
    "avg_phonemes_per_character_all_lemma_aw_text = []\n",
    "prior_prob_cons_lemma_aw_text = []\n",
    "max_prob_cons_lemma_aw_text = []\n",
    "min_prob_cons_lemma_aw_text = []\n",
    "mid_prob_cons_lemma_aw_text = []\n",
    "number_phonemes_cons_lemma_aw_text = []\n",
    "prior_prob_vowel_lemma_aw_text = []\n",
    "max_prob_vowel_lemma_aw_text = []\n",
    "min_prob_vowel_lemma_aw_text = []\n",
    "mid_prob_vowel_lemma_aw_text = []\n",
    "number_phonemes_vowel_lemma_aw_text = []\n",
    "prior_prob_all_lemma_aw_text = []\n",
    "max_prob_all_lemma_aw_text = []\n",
    "mid_prob_all_lemma_aw_text = []\n",
    "min_prob_all_lemma_aw_text = []\n",
    "number_phonemes_all_lemma_aw_text = []\n",
    "Conditional_Probability_Average_lemma_aw_text = []\n",
    "Ortho_N_lemma_aw_text = []\n",
    "Phono_N_lemma_aw_text = []\n",
    "Phono_N_H_lemma_aw_text = []\n",
    "OG_N_lemma_aw_text = []\n",
    "OG_N_H_lemma_aw_text = []\n",
    "Freq_N_lemma_aw_text = []\n",
    "Freq_N_P_lemma_aw_text = []\n",
    "Freq_N_PH_lemma_aw_text = []\n",
    "Freq_N_OG_lemma_aw_text = []\n",
    "Freq_N_OGH_lemma_aw_text = []\n",
    "OLD_lemma_aw_text = []\n",
    "OLDF_lemma_aw_text = []\n",
    "PLD_lemma_aw_text = []\n",
    "PLDF_lemma_aw_text = []\n",
    "subtlexus_log_freq_lemma_aw_text = []\n",
    "subtlexus_log_cd_lemma_aw_text = []\n",
    "coca_maga_cd_lemma_aw_text = []\n",
    "coca_mag_log_freq_lemma_aw_text = []\n",
    "num_rhymes_full_elp_lemma_aw_text = []\n",
    "num_rhymes_1000_coca_lemma_aw_text = []\n",
    "num_rhymes_2500_coca_lemma_aw_text = []\n",
    "num_rhymes_5000_coca_lemma_aw_text = []\n",
    "num_rhymes_10000_coca_lemma_aw_text = []\n",
    "\n",
    "for tokenized_doc in df_docs:\n",
    "    num_syllables_lemma_aw = []\n",
    "    num_letters_lemma_aw = []\n",
    "    num_phonemes_lemma_aw = []\n",
    "    discrepancy_raw_lemma_aw = []\n",
    "    discrepancy_ratio_lemma_aw = []\n",
    "    avg_syllable_length_lemma_aw = []\n",
    "    num_consonants_characters_lemma_aw = []\n",
    "    num_vowel_characters_lemma_aw = []\n",
    "    num_consonants_phonemes_lemma_aw = []\n",
    "    num_vowel_phonemes_lemma_aw = []\n",
    "    avg_phonemes_per_character_consonants_lemma_aw = []\n",
    "    avg_phonemes_per_character_vowels_lemma_aw = []\n",
    "    avg_phonemes_per_character_all_lemma_aw = []\n",
    "    prior_prob_cons_lemma_aw = []\n",
    "    max_prob_cons_lemma_aw = []\n",
    "    min_prob_cons_lemma_aw = []\n",
    "    mid_prob_cons_lemma_aw = []\n",
    "    number_phonemes_cons_lemma_aw = []\n",
    "    prior_prob_vowel_lemma_aw = []\n",
    "    max_prob_vowel_lemma_aw = []\n",
    "    min_prob_vowel_lemma_aw = []\n",
    "    mid_prob_vowel_lemma_aw = []\n",
    "    number_phonemes_vowel_lemma_aw = []\n",
    "    prior_prob_all_lemma_aw = []\n",
    "    max_prob_all_lemma_aw = []\n",
    "    mid_prob_all_lemma_aw = []\n",
    "    min_prob_all_lemma_aw = []\n",
    "    number_phonemes_all_lemma_aw = []\n",
    "    Conditional_Probability_Average_lemma_aw = []\n",
    "    Ortho_N_lemma_aw = []\n",
    "    Phono_N_lemma_aw = []\n",
    "    Phono_N_H_lemma_aw = []\n",
    "    OG_N_lemma_aw = []\n",
    "    OG_N_H_lemma_aw = []\n",
    "    Freq_N_lemma_aw = []\n",
    "    Freq_N_P_lemma_aw = []\n",
    "    Freq_N_PH_lemma_aw = []\n",
    "    Freq_N_OG_lemma_aw = []\n",
    "    Freq_N_OGH_lemma_aw = []\n",
    "    OLD_lemma_aw = []\n",
    "    OLDF_lemma_aw = []\n",
    "    PLD_lemma_aw = []\n",
    "    PLDF_lemma_aw = []\n",
    "    subtlexus_log_freq_lemma_aw = []\n",
    "    subtlexus_log_cd_lemma_aw = []\n",
    "    coca_maga_cd_lemma_aw = []\n",
    "    coca_mag_log_freq_lemma_aw = []\n",
    "    num_rhymes_full_elp_lemma_aw = []\n",
    "    num_rhymes_1000_coca_lemma_aw = []\n",
    "    num_rhymes_2500_coca_lemma_aw = []\n",
    "    num_rhymes_5000_coca_lemma_aw = []\n",
    "    num_rhymes_10000_coca_lemma_aw = []\n",
    "    for token in tokenized_doc:\n",
    "        if not token.is_punct:\n",
    "            #print(token.text, token.lemma_)\n",
    "            try:\n",
    "                val = decoding_dic[token.lemma_]\n",
    "                #print(key)\n",
    "                #print(val[0])\n",
    "                num_syllables_lemma_aw.append(val[0])\n",
    "                num_letters_lemma_aw.append(val[1])\n",
    "                num_phonemes_lemma_aw.append(val[2])\n",
    "                discrepancy_raw_lemma_aw.append(val[3])\n",
    "                discrepancy_ratio_lemma_aw.append(val[4])\n",
    "                avg_syllable_length_lemma_aw.append(val[5])\n",
    "                num_consonants_characters_lemma_aw.append(val[6])\n",
    "                num_vowel_characters_lemma_aw.append(val[7])\n",
    "                num_consonants_phonemes_lemma_aw.append(val[8])\n",
    "                num_vowel_phonemes_lemma_aw.append(val[9])\n",
    "                avg_phonemes_per_character_consonants_lemma_aw.append(val[10])\n",
    "                avg_phonemes_per_character_vowels_lemma_aw.append(val[11])\n",
    "                avg_phonemes_per_character_all_lemma_aw.append(val[12])\n",
    "                prior_prob_cons_lemma_aw.append(val[13])\n",
    "                max_prob_cons_lemma_aw.append(val[14])\n",
    "                min_prob_cons_lemma_aw.append(val[15])\n",
    "                mid_prob_cons_lemma_aw.append(val[16])\n",
    "                number_phonemes_cons_lemma_aw.append(val[17])\n",
    "                prior_prob_vowel_lemma_aw.append(val[18])\n",
    "                max_prob_vowel_lemma_aw.append(val[19])\n",
    "                min_prob_vowel_lemma_aw.append(val[20])\n",
    "                mid_prob_vowel_lemma_aw.append(val[21])\n",
    "                number_phonemes_vowel_lemma_aw.append(val[22])\n",
    "                prior_prob_all_lemma_aw.append(val[23])\n",
    "                max_prob_all_lemma_aw.append(val[24])\n",
    "                mid_prob_all_lemma_aw.append(val[25])\n",
    "                min_prob_all_lemma_aw.append(val[26])\n",
    "                number_phonemes_all_lemma_aw.append(val[27])\n",
    "                Conditional_Probability_Average_lemma_aw.append(val[28])\n",
    "                Ortho_N_lemma_aw.append(val[29])\n",
    "                Phono_N_lemma_aw.append(val[30])\n",
    "                Phono_N_H_lemma_aw.append(val[31])\n",
    "                OG_N_lemma_aw.append(val[32])\n",
    "                OG_N_H_lemma_aw.append(val[33])\n",
    "                Freq_N_lemma_aw.append(val[34])\n",
    "                Freq_N_P_lemma_aw.append(val[35])\n",
    "                Freq_N_PH_lemma_aw.append(val[36])\n",
    "                Freq_N_OG_lemma_aw.append(val[37])\n",
    "                Freq_N_OGH_lemma_aw.append(val[38])\n",
    "                OLD_lemma_aw.append(val[39])\n",
    "                OLDF_lemma_aw.append(val[40])\n",
    "                PLD_lemma_aw.append(val[41])\n",
    "                PLDF_lemma_aw.append(val[42])\n",
    "                subtlexus_log_freq_lemma_aw.append(val[43])\n",
    "                subtlexus_log_cd_lemma_aw.append(val[44])\n",
    "                coca_maga_cd_lemma_aw.append(val[45])\n",
    "                coca_mag_log_freq_lemma_aw.append(val[46])\n",
    "                num_rhymes_full_elp_lemma_aw.append(val[47])\n",
    "                num_rhymes_1000_coca_lemma_aw.append(val[48])\n",
    "                num_rhymes_2500_coca_lemma_aw.append(val[49])\n",
    "                num_rhymes_5000_coca_lemma_aw.append(val[50])\n",
    "                num_rhymes_10000_coca_lemma_aw.append(val[51])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    num_syllables_lemma_aw_text.append(num_syllables_lemma_aw)\n",
    "    num_letters_lemma_aw_text.append(num_letters_lemma_aw)\n",
    "    num_phonemes_lemma_aw_text.append(num_phonemes_lemma_aw)\n",
    "    discrepancy_raw_lemma_aw_text.append(discrepancy_raw_lemma_aw)\n",
    "    discrepancy_ratio_lemma_aw_text.append(discrepancy_ratio_lemma_aw)\n",
    "    avg_syllable_length_lemma_aw_text.append(avg_syllable_length_lemma_aw)\n",
    "    num_consonants_characters_lemma_aw_text.append(num_consonants_characters_lemma_aw)\n",
    "    num_vowel_characters_lemma_aw_text.append(num_vowel_characters_lemma_aw)\n",
    "    num_consonants_phonemes_lemma_aw_text.append(num_consonants_phonemes_lemma_aw)\n",
    "    num_vowel_phonemes_lemma_aw_text.append(num_vowel_phonemes_lemma_aw)\n",
    "    avg_phonemes_per_character_consonants_lemma_aw_text.append(avg_phonemes_per_character_consonants_lemma_aw)\n",
    "    avg_phonemes_per_character_vowels_lemma_aw_text.append(avg_phonemes_per_character_vowels_lemma_aw)\n",
    "    avg_phonemes_per_character_all_lemma_aw_text.append(avg_phonemes_per_character_all_lemma_aw)\n",
    "    prior_prob_cons_lemma_aw_text.append(prior_prob_cons_lemma_aw)\n",
    "    max_prob_cons_lemma_aw_text.append(max_prob_cons_lemma_aw)\n",
    "    min_prob_cons_lemma_aw_text.append(min_prob_cons_lemma_aw)\n",
    "    mid_prob_cons_lemma_aw_text.append(mid_prob_cons_lemma_aw)\n",
    "    number_phonemes_cons_lemma_aw_text.append(number_phonemes_cons_lemma_aw)\n",
    "    prior_prob_vowel_lemma_aw_text.append(prior_prob_vowel_lemma_aw)\n",
    "    max_prob_vowel_lemma_aw_text.append(max_prob_vowel_lemma_aw)\n",
    "    min_prob_vowel_lemma_aw_text.append(min_prob_vowel_lemma_aw)\n",
    "    mid_prob_vowel_lemma_aw_text.append(mid_prob_vowel_lemma_aw)\n",
    "    number_phonemes_vowel_lemma_aw_text.append(number_phonemes_vowel_lemma_aw)\n",
    "    prior_prob_all_lemma_aw_text.append(prior_prob_all_lemma_aw)\n",
    "    max_prob_all_lemma_aw_text.append(max_prob_all_lemma_aw)\n",
    "    mid_prob_all_lemma_aw_text.append(mid_prob_all_lemma_aw)\n",
    "    min_prob_all_lemma_aw_text.append(min_prob_all_lemma_aw)\n",
    "    number_phonemes_all_lemma_aw_text.append(number_phonemes_all_lemma_aw)\n",
    "    Conditional_Probability_Average_lemma_aw_text.append(Conditional_Probability_Average_lemma_aw)\n",
    "    Ortho_N_lemma_aw_text.append(Ortho_N_lemma_aw)\n",
    "    Phono_N_lemma_aw_text.append(Phono_N_lemma_aw)\n",
    "    Phono_N_H_lemma_aw_text.append(Phono_N_H_lemma_aw)\n",
    "    OG_N_lemma_aw_text.append(OG_N_lemma_aw)\n",
    "    OG_N_H_lemma_aw_text.append(OG_N_H_lemma_aw)\n",
    "    Freq_N_lemma_aw_text.append(Freq_N_lemma_aw)\n",
    "    Freq_N_P_lemma_aw_text.append(Freq_N_P_lemma_aw)\n",
    "    Freq_N_PH_lemma_aw_text.append(Freq_N_PH_lemma_aw)\n",
    "    Freq_N_OG_lemma_aw_text.append(Freq_N_OG_lemma_aw)\n",
    "    Freq_N_OGH_lemma_aw_text.append(Freq_N_OGH_lemma_aw)\n",
    "    OLD_lemma_aw_text.append(OLD_lemma_aw)\n",
    "    OLDF_lemma_aw_text.append(OLDF_lemma_aw)\n",
    "    PLD_lemma_aw_text.append(PLD_lemma_aw)\n",
    "    PLDF_lemma_aw_text.append(PLDF_lemma_aw)\n",
    "    subtlexus_log_freq_lemma_aw_text.append(subtlexus_log_freq_lemma_aw)\n",
    "    subtlexus_log_cd_lemma_aw_text.append(subtlexus_log_cd_lemma_aw)\n",
    "    coca_maga_cd_lemma_aw_text.append(coca_maga_cd_lemma_aw)\n",
    "    coca_mag_log_freq_lemma_aw_text.append(coca_mag_log_freq_lemma_aw)\n",
    "    num_rhymes_full_elp_lemma_aw_text.append(num_rhymes_full_elp_lemma_aw)\n",
    "    num_rhymes_1000_coca_lemma_aw_text.append(num_rhymes_1000_coca_lemma_aw)\n",
    "    num_rhymes_2500_coca_lemma_aw_text.append(num_rhymes_2500_coca_lemma_aw)\n",
    "    num_rhymes_5000_coca_lemma_aw_text.append(num_rhymes_5000_coca_lemma_aw)\n",
    "    num_rhymes_10000_coca_lemma_aw_text.append(num_rhymes_10000_coca_lemma_aw)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "df001800-91da-4376-a0bd-9e51ec56a72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0            this is a simple practice text with ghlq.\n",
      "1                                   same, but simpler.\n",
      "2    words increasing, i think. about aaberg and aa...\n",
      "3                                          it will be.\n",
      "Name: Excerpt, dtype: object\n",
      "this is a list of number of syllables for each lemma in the text [[1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0], [1.0, 1.0, 2.0], [1.0, 2.0, 1.0, 2.0, 2.0, 1.0, 3.0, 3.0], [1.0, 1.0, 1.0]]\n",
      "this is a list of number of syllables for each word in the text [[1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0], [1.0, 1.0, 3.0], [1.0, 3.0, 1.0, 1.0, 2.0, 2.0, 1.0, 3.0, 3.0], [1.0, 1.0, 1.0]]\n",
      "this is a list of number of letters for each lemma in the text [[4.0, 2.0, 1.0, 6.0, 8.0, 4.0, 4.0], [4.0, 3.0, 6.0], [4.0, 8.0, 5.0, 5.0, 6.0, 3.0, 3.0, 7.0], [2.0, 4.0, 2.0]]\n",
      "this is a list of number of letters for each word in the text [[4.0, 2.0, 1.0, 6.0, 8.0, 4.0, 4.0], [4.0, 3.0, 7.0], [5.0, 10.0, 1.0, 5.0, 5.0, 6.0, 3.0, 3.0, 7.0], [2.0, 4.0, 2.0]]\n"
     ]
    }
   ],
   "source": [
    "print(df['Excerpt'])\n",
    "\n",
    "#sanity checks\n",
    "print(f'this is a list of number of syllables for each lemma in the text {num_syllables_lemma_aw_text}')\n",
    "print(f'this is a list of number of syllables for each word in the text {num_syllables_token_aw_text}')\n",
    "\n",
    "print(f'this is a list of number of letters for each lemma in the text {num_letters_lemma_aw_text}')\n",
    "print(f'this is a list of number of letters for each word in the text {num_letters_token_aw_text}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fc81a9f6-78a4-475c-90bf-f1a7aa585869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove nan's from the list of lists\n",
    "\n",
    "Conditional_Probability_Average_lemma_aw_text_no_nan = remove_nan(Conditional_Probability_Average_lemma_aw_text)\n",
    "Ortho_N_lemma_aw_text_no_nan = remove_nan(Ortho_N_lemma_aw_text)\n",
    "Phono_N_lemma_aw_text_no_nan = remove_nan(Phono_N_lemma_aw_text)\n",
    "Phono_N_H_lemma_aw_text_no_nan = remove_nan(Phono_N_H_lemma_aw_text)\n",
    "OG_N_lemma_aw_text_no_nan = remove_nan(OG_N_lemma_aw_text)\n",
    "OG_N_H_lemma_aw_text_no_nan = remove_nan(OG_N_H_lemma_aw_text)\n",
    "Freq_N_lemma_aw_text_no_nan = remove_nan(Freq_N_lemma_aw_text)\n",
    "Freq_N_P_lemma_aw_text_no_nan = remove_nan(Freq_N_P_lemma_aw_text)\n",
    "Freq_N_PH_lemma_aw_text_no_nan = remove_nan(Freq_N_PH_lemma_aw_text)\n",
    "Freq_N_OG_lemma_aw_text_no_nan = remove_nan(Freq_N_OG_lemma_aw_text)\n",
    "Freq_N_OGH_lemma_aw_text_no_nan = remove_nan(Freq_N_OGH_lemma_aw_text)\n",
    "OLD_lemma_aw_text_no_nan = remove_nan(OLD_lemma_aw_text)\n",
    "OLDF_lemma_aw_text_no_nan = remove_nan(OLDF_lemma_aw_text)\n",
    "PLD_lemma_aw_text_no_nan = remove_nan(PLD_lemma_aw_text)\n",
    "PLDF_lemma_aw_text_no_nan = remove_nan(PLDF_lemma_aw_text)\n",
    "subtlexus_log_freq_lemma_aw_text_no_nan = remove_nan(subtlexus_log_freq_lemma_aw_text)\n",
    "subtlexus_log_cd_lemma_aw_text_no_nan = remove_nan(subtlexus_log_cd_lemma_aw_text)\n",
    "coca_maga_cd_lemma_aw_text_no_nan = remove_nan(coca_maga_cd_lemma_aw_text)\n",
    "coca_mag_log_freq_lemma_aw_text_no_nan = remove_nan(coca_mag_log_freq_lemma_aw_text)\n",
    "num_rhymes_full_elp_lemma_aw_text_no_nan = remove_nan(num_rhymes_full_elp_lemma_aw_text)\n",
    "num_rhymes_1000_coca_lemma_aw_text_no_nan = remove_nan(num_rhymes_1000_coca_lemma_aw_text)\n",
    "num_rhymes_2500_coca_lemma_aw_text_no_nan = remove_nan(num_rhymes_2500_coca_lemma_aw_text)\n",
    "num_rhymes_5000_coca_lemma_aw_text_no_nan = remove_nan(num_rhymes_5000_coca_lemma_aw_text)\n",
    "num_rhymes_10000_coca_lemma_aw_text_no_nan = remove_nan(num_rhymes_10000_coca_lemma_aw_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "69023629-69a3-48c8-a98a-c0054b2f09a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#get lists that are average of sublists\n",
    "\n",
    "num_syl_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_syllables_lemma_aw_text]#if it is a sublist, get average, else (if it is not a sublist, empty list, return 0) \n",
    "num_let_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_letters_lemma_aw_text]\n",
    "num_phone_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_phonemes_lemma_aw_text]\n",
    "discrepancy_raw_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in discrepancy_raw_lemma_aw_text]\n",
    "discrepancy_ratio_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in discrepancy_ratio_lemma_aw_text]\n",
    "avg_syl_length_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in avg_syllable_length_lemma_aw_text]\n",
    "num_cons_char_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_consonants_characters_lemma_aw_text]\n",
    "num_vowel_char_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_vowel_characters_lemma_aw_text]\n",
    "num_cons_phone_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_consonants_phonemes_lemma_aw_text]\n",
    "num_vowel_phone_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_vowel_phonemes_lemma_aw_text]\n",
    "avg_phone_per_char_cons_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in avg_phonemes_per_character_consonants_lemma_aw_text]\n",
    "avg_phone_per_char_vowel_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in avg_phonemes_per_character_vowels_lemma_aw_text]\n",
    "avg_phone_per_char_all_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in avg_phonemes_per_character_all_lemma_aw_text]\n",
    "prior_prob_cons_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in prior_prob_cons_lemma_aw_text]\n",
    "max_prob_cons_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in max_prob_cons_lemma_aw_text]\n",
    "min_prob_cons_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in min_prob_cons_lemma_aw_text]\n",
    "mid_prob_cons_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in mid_prob_cons_lemma_aw_text]\n",
    "number_phone_cons_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in number_phonemes_cons_lemma_aw_text]\n",
    "prior_prob_vowel_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in prior_prob_vowel_lemma_aw_text]\n",
    "max_prob_vowel_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in max_prob_vowel_lemma_aw_text]\n",
    "min_prob_vowel_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in min_prob_vowel_lemma_aw_text]\n",
    "mid_prob_vowel_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in mid_prob_vowel_lemma_aw_text]\n",
    "number_phone_vowel_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in number_phonemes_vowel_lemma_aw_text]\n",
    "prior_prob_all_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in prior_prob_all_lemma_aw_text]\n",
    "max_prob_all_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in max_prob_all_lemma_aw_text]\n",
    "mid_prob_all_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in mid_prob_all_lemma_aw_text]\n",
    "min_prob_all_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in min_prob_all_lemma_aw_text]\n",
    "number_phone_all_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in number_phonemes_all_lemma_aw_text]\n",
    "Conditional_Probability_Average_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in Conditional_Probability_Average_lemma_aw_text_no_nan]\n",
    "Ortho_N_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in Ortho_N_lemma_aw_text_no_nan]\n",
    "Phono_N_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in Phono_N_lemma_aw_text_no_nan]\n",
    "Phono_N_H_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in Phono_N_H_lemma_aw_text_no_nan]\n",
    "OG_N_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in OG_N_lemma_aw_text_no_nan]\n",
    "OG_N_H_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in OG_N_H_lemma_aw_text_no_nan]\n",
    "Freq_N_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in Freq_N_lemma_aw_text_no_nan]\n",
    "Freq_N_P_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in Freq_N_P_lemma_aw_text_no_nan]\n",
    "Freq_N_PH_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in Freq_N_PH_lemma_aw_text_no_nan]\n",
    "Freq_N_OG_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in Freq_N_OG_lemma_aw_text_no_nan]\n",
    "Freq_N_OGH_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in Freq_N_OGH_lemma_aw_text_no_nan]\n",
    "OLD_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in OLD_lemma_aw_text_no_nan]\n",
    "OLDF_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in OLDF_lemma_aw_text_no_nan]\n",
    "PLD_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in PLD_lemma_aw_text_no_nan]\n",
    "PLDF_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in PLDF_lemma_aw_text_no_nan]\n",
    "subtlexus_log_freq_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in subtlexus_log_freq_lemma_aw_text_no_nan]\n",
    "subtlexus_log_cd_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in subtlexus_log_cd_lemma_aw_text_no_nan]\n",
    "coca_maga_cd_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in coca_maga_cd_lemma_aw_text_no_nan]\n",
    "coca_mag_log_freq_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in coca_mag_log_freq_lemma_aw_text_no_nan]\n",
    "num_rhymes_full_elp_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_rhymes_full_elp_lemma_aw_text_no_nan]\n",
    "num_rhymes_1000_coca_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_rhymes_1000_coca_lemma_aw_text_no_nan]\n",
    "num_rhymes_2500_coca_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_rhymes_2500_coca_lemma_aw_text_no_nan]\n",
    "num_rhymes_5000_coca_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_rhymes_5000_coca_lemma_aw_text_no_nan]\n",
    "num_rhymes_10000_coca_lemma_aw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_rhymes_10000_coca_lemma_aw_text_no_nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cd3f0621-0e5e-4252-bd29-a651733b9dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add to database\n",
    "\n",
    "df2 = df2.assign(num_syl_lemma_aw = num_syl_lemma_aw, \n",
    "                num_let_lemma_aw = num_let_lemma_aw, \n",
    "                num_phone_lemma_aw = num_phone_lemma_aw, \n",
    "                discrepancy_raw_lemma_aw = discrepancy_raw_lemma_aw, \n",
    "                discrepancy_ratio_lemma_aw = discrepancy_ratio_lemma_aw, \n",
    "                avg_syl_length_lemma_aw = avg_syl_length_lemma_aw, \n",
    "                num_cons_char_lemma_aw = num_cons_char_lemma_aw, \n",
    "                num_vowel_char_lemma_aw = num_vowel_char_lemma_aw, \n",
    "                num_cons_phone_lemma_aw = num_cons_phone_lemma_aw, \n",
    "                num_vowel_phone_lemma_aw = num_vowel_phone_lemma_aw, \n",
    "                avg_phone_per_char_cons_lemma_aw = avg_phone_per_char_cons_lemma_aw, \n",
    "                avg_phone_per_char_vowel_lemma_aw = avg_phone_per_char_vowel_lemma_aw, \n",
    "                avg_phone_per_char_all_lemma_aw = avg_phone_per_char_all_lemma_aw, \n",
    "                reverse_prior_prob_cons_lemma_aw = prior_prob_cons_lemma_aw, \n",
    "                max_prob_cons_lemma_aw = max_prob_cons_lemma_aw, \n",
    "                min_prob_cons_lemma_aw = min_prob_cons_lemma_aw, \n",
    "                mid_prob_cons_lemma_aw = mid_prob_cons_lemma_aw, \n",
    "                number_phone_cons_lemma_aw = number_phone_cons_lemma_aw, \n",
    "                reverse_prior_prob_vowel_lemma_aw = prior_prob_vowel_lemma_aw, \n",
    "                max_prob_vowel_lemma_aw = max_prob_vowel_lemma_aw, \n",
    "                min_prob_vowel_lemma_aw = min_prob_vowel_lemma_aw, \n",
    "                mid_prob_vowel_lemma_aw = mid_prob_vowel_lemma_aw, \n",
    "                number_phone_vowel_lemma_aw = number_phone_vowel_lemma_aw, \n",
    "                reverse_prior_prob_all_lemma_aw = prior_prob_all_lemma_aw, \n",
    "                max_prob_all_lemma_aw = max_prob_all_lemma_aw, \n",
    "                mid_prob_all_lemma_aw = mid_prob_all_lemma_aw, \n",
    "                min_prob_all_lemma_aw = min_prob_all_lemma_aw, \n",
    "                number_phone_all_lemma_aw = number_phone_all_lemma_aw,\n",
    "                Conditional_Probability_Average_lemma_aw = Conditional_Probability_Average_lemma_aw,\n",
    "                Ortho_N_lemma_aw = Ortho_N_lemma_aw,\n",
    "                Phono_N_lemma_aw = Phono_N_lemma_aw,\n",
    "                Phono_N_H_lemma_aw = Phono_N_H_lemma_aw,\n",
    "                OG_N_lemma_aw = OG_N_lemma_aw,\n",
    "                OG_N_H_lemma_aw = OG_N_H_lemma_aw,\n",
    "                Freq_N_lemma_aw = Freq_N_lemma_aw,\n",
    "                Freq_N_P_lemma_aw = Freq_N_P_lemma_aw,\n",
    "                Freq_N_PH_lemma_aw = Freq_N_PH_lemma_aw,\n",
    "                Freq_N_OG_lemma_aw = Freq_N_OG_lemma_aw,\n",
    "                Freq_N_OGH_lemma_aw = Freq_N_OGH_lemma_aw,\n",
    "                OLD_lemma_aw = OLD_lemma_aw,\n",
    "                OLDF_lemma_aw = OLDF_lemma_aw,\n",
    "                PLD_lemma_aw = PLD_lemma_aw,\n",
    "                PLDF_lemma_aw = PLDF_lemma_aw,\n",
    "                subtlexus_log_freq_lemma_aw = subtlexus_log_freq_lemma_aw,\n",
    "                subtlexus_log_cd_lemma_aw = subtlexus_log_cd_lemma_aw,\n",
    "                coca_maga_cd_lemma_aw = coca_maga_cd_lemma_aw,\n",
    "                coca_mag_log_freq_lemma_aw = coca_mag_log_freq_lemma_aw,\n",
    "                num_rhymes_full_elp_lemma_aw = num_rhymes_full_elp_lemma_aw,\n",
    "                num_rhymes_1000_coca_lemma_aw = num_rhymes_1000_coca_lemma_aw,\n",
    "                num_rhymes_2500_coca_lemma_aw = num_rhymes_2500_coca_lemma_aw,\n",
    "                num_rhymes_5000_coca_lemma_aw = num_rhymes_5000_coca_lemma_aw,\n",
    "                num_rhymes_10000_coca_lemma_aw = num_rhymes_10000_coca_lemma_aw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6b3c2854-4fe1-4922-9d27-28b230d0468f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 203)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77e807a-b3b6-4ed8-82a5-915782b24f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f65b509b-5968-4e4e-92ca-56293df26129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4 entries, 0 to 3\n",
      "Data columns (total 203 columns):\n",
      " #    Column                                    Non-Null Count  Dtype  \n",
      "---   ------                                    --------------  -----  \n",
      " 0    ID                                        4 non-null      int64  \n",
      " 1    Author                                    4 non-null      object \n",
      " 2    Title                                     4 non-null      object \n",
      " 3    Anthology                                 0 non-null      float64\n",
      " 4    URL                                       4 non-null      object \n",
      " 5    Pub Year                                  4 non-null      int64  \n",
      " 6    Categ                                     4 non-null      object \n",
      " 7    Sub Cat                                   0 non-null      float64\n",
      " 8    Lexile Band                               4 non-null      int64  \n",
      " 9    Location                                  4 non-null      object \n",
      " 10   License                                   0 non-null      float64\n",
      " 11   MPAA Max                                  4 non-null      object \n",
      " 12   MPAA #Max                                 4 non-null      int64  \n",
      " 13   MPAA# Avg                                 4 non-null      float64\n",
      " 14   Excerpt                                   4 non-null      object \n",
      " 15   Google WC                                 4 non-null      int64  \n",
      " 16   Sentence Count                            4 non-null      int64  \n",
      " 17   Paragraphs                                4 non-null      int64  \n",
      " 18   BT_easiness                               4 non-null      float64\n",
      " 19   s.e.                                      4 non-null      float64\n",
      " 20   Flesch-Reading-Ease                       4 non-null      float64\n",
      " 21   Flesch-Kincaid-Grade-Level                4 non-null      float64\n",
      " 22   Automated Readability Index               4 non-null      float64\n",
      " 23   SMOG Readability                          4 non-null      int64  \n",
      " 24   New Dale-Chall Readability Formula        4 non-null      float64\n",
      " 25   CAREC                                     4 non-null      float64\n",
      " 26   CAREC_M                                   4 non-null      float64\n",
      " 27   CML2RI                                    4 non-null      float64\n",
      " 28   nw_token                                  4 non-null      int64  \n",
      " 29   num_syl_tok_aw                            4 non-null      float64\n",
      " 30   num_let_tok_aw                            4 non-null      float64\n",
      " 31   num_phone_tok_aw                          4 non-null      float64\n",
      " 32   discrepancy_raw_tok_aw                    4 non-null      float64\n",
      " 33   discrepancy_ratio_tok_aw                  4 non-null      float64\n",
      " 34   avg_syl_length_tok_aw                     4 non-null      float64\n",
      " 35   num_cons_char_tok_aw                      4 non-null      float64\n",
      " 36   num_vowel_char_tok_aw                     4 non-null      float64\n",
      " 37   num_cons_phone_tok_aw                     4 non-null      float64\n",
      " 38   num_vowel_phone_tok_aw                    4 non-null      float64\n",
      " 39   avg_phone_per_char_cons_tok_aw            4 non-null      float64\n",
      " 40   avg_phone_per_char_vowel_tok_aw           4 non-null      float64\n",
      " 41   avg_phone_per_char_all_tok_aw             4 non-null      float64\n",
      " 42   reverse_prior_prob_cons_tok_aw            4 non-null      float64\n",
      " 43   max_prob_cons_tok_aw                      4 non-null      float64\n",
      " 44   min_prob_cons_tok_aw                      4 non-null      float64\n",
      " 45   mid_prob_cons_tok_aw                      4 non-null      float64\n",
      " 46   number_phone_cons_tok_aw                  4 non-null      float64\n",
      " 47   reverse_prior_prob_vowel_tok_aw           4 non-null      float64\n",
      " 48   max_prob_vowel_tok_aw                     4 non-null      float64\n",
      " 49   min_prob_vowel_tok_aw                     4 non-null      float64\n",
      " 50   mid_prob_vowel_tok_aw                     4 non-null      float64\n",
      " 51   number_phone_vowel_tok_aw                 4 non-null      float64\n",
      " 52   reverse_prior_prob_all_tok_aw             4 non-null      float64\n",
      " 53   max_prob_all_tok_aw                       4 non-null      float64\n",
      " 54   mid_prob_all_tok_aw                       4 non-null      float64\n",
      " 55   min_prob_all_tok_aw                       4 non-null      float64\n",
      " 56   number_phone_all_tok_aw                   4 non-null      float64\n",
      " 57   Conditional_Probability_Average_tok_aw    4 non-null      float64\n",
      " 58   Ortho_N_tok_aw                            4 non-null      float64\n",
      " 59   Phono_N_tok_aw                            4 non-null      float64\n",
      " 60   Phono_N_H_tok_aw                          4 non-null      float64\n",
      " 61   OG_N_tok_aw                               4 non-null      float64\n",
      " 62   OG_N_H_tok_aw                             4 non-null      float64\n",
      " 63   Freq_N_tok_aw                             4 non-null      float64\n",
      " 64   Freq_N_P_tok_aw                           4 non-null      float64\n",
      " 65   Freq_N_PH_tok_aw                          4 non-null      float64\n",
      " 66   Freq_N_OG_tok_aw                          4 non-null      float64\n",
      " 67   Freq_N_OGH_tok_aw                         4 non-null      float64\n",
      " 68   OLD_tok_aw                                4 non-null      float64\n",
      " 69   OLDF_tok_aw                               4 non-null      float64\n",
      " 70   PLD_tok_aw                                4 non-null      float64\n",
      " 71   PLDF_tok_aw                               4 non-null      float64\n",
      " 72   subtlexus_log_freq_tok_aw                 4 non-null      float64\n",
      " 73   subtlexus_log_cd_tok_aw                   4 non-null      float64\n",
      " 74   coca_maga_cd_tok_aw                       4 non-null      float64\n",
      " 75   coca_mag_log_freq_tok_aw                  4 non-null      float64\n",
      " 76   num_rhymes_full_elp_tok_aw                4 non-null      float64\n",
      " 77   num_rhymes_1000_coca_tok_aw               4 non-null      float64\n",
      " 78   num_rhymes_2500_coca_tok_aw               4 non-null      float64\n",
      " 79   num_rhymes_5000_coca_tok_aw               4 non-null      float64\n",
      " 80   num_rhymes_10000_coca_tok_aw              4 non-null      float64\n",
      " 81   weight_max_prob_cons_tok_aw               4 non-null      float64\n",
      " 82   weight_min_prob_cons_tok_aw               4 non-null      float64\n",
      " 83   weight_mid_prob_cons_tok_aw               4 non-null      float64\n",
      " 84   weight_max_prob_vowel_tok_aw              4 non-null      float64\n",
      " 85   weight_min_prob_vowel_tok_aw              4 non-null      float64\n",
      " 86   weight_mid_prob_vowel_tok_aw              4 non-null      float64\n",
      " 87   weight_max_prob_all_tok_aw                4 non-null      float64\n",
      " 88   weight_mid_prob_all_tok_aw                4 non-null      float64\n",
      " 89   weight_min_prob_all_tok_aw                4 non-null      float64\n",
      " 90   num_syl_tok_cw                            4 non-null      float64\n",
      " 91   num_let_tok_cw                            4 non-null      float64\n",
      " 92   num_phone_tok_cw                          4 non-null      float64\n",
      " 93   discrepancy_raw_tok_cw                    4 non-null      float64\n",
      " 94   discrepancy_ratio_tok_cw                  4 non-null      float64\n",
      " 95   avg_syl_length_tok_cw                     4 non-null      float64\n",
      " 96   num_cons_char_tok_cw                      4 non-null      float64\n",
      " 97   num_vowel_char_tok_cw                     4 non-null      float64\n",
      " 98   num_cons_phone_tok_cw                     4 non-null      float64\n",
      " 99   num_vowel_phone_tok_cw                    4 non-null      float64\n",
      " 100  avg_phone_per_char_cons_tok_cw            4 non-null      float64\n",
      " 101  avg_phone_per_char_vowel_tok_cw           4 non-null      float64\n",
      " 102  avg_phone_per_char_all_tok_cw             4 non-null      float64\n",
      " 103  reverse_prior_prob_cons_tok_cw            4 non-null      float64\n",
      " 104  max_prob_cons_tok_cw                      4 non-null      float64\n",
      " 105  min_prob_cons_tok_cw                      4 non-null      float64\n",
      " 106  mid_prob_cons_tok_cw                      4 non-null      float64\n",
      " 107  number_phone_cons_tok_cw                  4 non-null      float64\n",
      " 108  reverse_prior_prob_vowel_tok_cw           4 non-null      float64\n",
      " 109  max_prob_vowel_tok_cw                     4 non-null      float64\n",
      " 110  min_prob_vowel_tok_cw                     4 non-null      float64\n",
      " 111  mid_prob_vowel_tok_cw                     4 non-null      float64\n",
      " 112  number_phone_vowel_tok_cw                 4 non-null      float64\n",
      " 113  reverse_prior_prob_all_tok_cw             4 non-null      float64\n",
      " 114  max_prob_all_tok_cw                       4 non-null      float64\n",
      " 115  mid_prob_all_tok_cw                       4 non-null      float64\n",
      " 116  min_prob_all_tok_cw                       4 non-null      float64\n",
      " 117  number_phone_all_tok_cw                   4 non-null      float64\n",
      " 118  Conditional_Probability_Average_tok_cw    4 non-null      float64\n",
      " 119  Ortho_N_tok_cw                            4 non-null      float64\n",
      " 120  Phono_N_tok_cw                            4 non-null      float64\n",
      " 121  Phono_N_H_tok_cw                          4 non-null      float64\n",
      " 122  OG_N_tok_cw                               4 non-null      float64\n",
      " 123  OG_N_H_tok_cw                             4 non-null      float64\n",
      " 124  Freq_N_tok_cw                             4 non-null      float64\n",
      " 125  Freq_N_P_tok_cw                           4 non-null      float64\n",
      " 126  Freq_N_PH_tok_cw                          4 non-null      float64\n",
      " 127  Freq_N_OG_tok_cw                          4 non-null      float64\n",
      " 128  Freq_N_OGH_tok_cw                         4 non-null      float64\n",
      " 129  OLD_tok_cw                                4 non-null      float64\n",
      " 130  OLDF_tok_cw                               4 non-null      float64\n",
      " 131  PLD_tok_cw                                4 non-null      float64\n",
      " 132  PLDF_tok_cw                               4 non-null      float64\n",
      " 133  subtlexus_log_freq_tok_cw                 4 non-null      float64\n",
      " 134  subtlexus_log_cd_tok_cw                   4 non-null      float64\n",
      " 135  coca_maga_cd_tok_cw                       4 non-null      float64\n",
      " 136  coca_mag_log_freq_tok_cw                  4 non-null      float64\n",
      " 137  num_rhymes_full_elp_tok_cw                4 non-null      float64\n",
      " 138  num_rhymes_1000_coca_tok_cw               4 non-null      float64\n",
      " 139  num_rhymes_2500_coca_tok_cw               4 non-null      float64\n",
      " 140  num_rhymes_5000_coca_tok_cw               4 non-null      float64\n",
      " 141  num_rhymes_10000_coca_tok_cw              4 non-null      float64\n",
      " 142  weight_max_prob_cons_tok_cw               3 non-null      float64\n",
      " 143  weight_min_prob_cons_tok_cw               3 non-null      float64\n",
      " 144  weight_mid_prob_cons_tok_cw               3 non-null      float64\n",
      " 145  weight_max_prob_vowel_tok_cw              3 non-null      float64\n",
      " 146  weight_min_prob_vowel_tok_cw              3 non-null      float64\n",
      " 147  weight_mid_prob_vowel_tok_cw              3 non-null      float64\n",
      " 148  weight_max_prob_all_tok_cw                3 non-null      float64\n",
      " 149  weight_mid_prob_all_tok_cw                3 non-null      float64\n",
      " 150  weight_min_prob_all_tok_cw                3 non-null      float64\n",
      " 151  num_syl_lemma_aw                          4 non-null      float64\n",
      " 152  num_let_lemma_aw                          4 non-null      float64\n",
      " 153  num_phone_lemma_aw                        4 non-null      float64\n",
      " 154  discrepancy_raw_lemma_aw                  4 non-null      float64\n",
      " 155  discrepancy_ratio_lemma_aw                4 non-null      float64\n",
      " 156  avg_syl_length_lemma_aw                   4 non-null      float64\n",
      " 157  num_cons_char_lemma_aw                    4 non-null      float64\n",
      " 158  num_vowel_char_lemma_aw                   4 non-null      float64\n",
      " 159  num_cons_phone_lemma_aw                   4 non-null      float64\n",
      " 160  num_vowel_phone_lemma_aw                  4 non-null      float64\n",
      " 161  avg_phone_per_char_cons_lemma_aw          4 non-null      float64\n",
      " 162  avg_phone_per_char_vowel_lemma_aw         4 non-null      float64\n",
      " 163  avg_phone_per_char_all_lemma_aw           4 non-null      float64\n",
      " 164  reverse_prior_prob_cons_lemma_aw          4 non-null      float64\n",
      " 165  max_prob_cons_lemma_aw                    4 non-null      float64\n",
      " 166  min_prob_cons_lemma_aw                    4 non-null      float64\n",
      " 167  mid_prob_cons_lemma_aw                    4 non-null      float64\n",
      " 168  number_phone_cons_lemma_aw                4 non-null      float64\n",
      " 169  reverse_prior_prob_vowel_lemma_aw         4 non-null      float64\n",
      " 170  max_prob_vowel_lemma_aw                   4 non-null      float64\n",
      " 171  min_prob_vowel_lemma_aw                   4 non-null      float64\n",
      " 172  mid_prob_vowel_lemma_aw                   4 non-null      float64\n",
      " 173  number_phone_vowel_lemma_aw               4 non-null      float64\n",
      " 174  reverse_prior_prob_all_lemma_aw           4 non-null      float64\n",
      " 175  max_prob_all_lemma_aw                     4 non-null      float64\n",
      " 176  mid_prob_all_lemma_aw                     4 non-null      float64\n",
      " 177  min_prob_all_lemma_aw                     4 non-null      float64\n",
      " 178  number_phone_all_lemma_aw                 4 non-null      float64\n",
      " 179  Conditional_Probability_Average_lemma_aw  4 non-null      float64\n",
      " 180  Ortho_N_lemma_aw                          4 non-null      float64\n",
      " 181  Phono_N_lemma_aw                          4 non-null      float64\n",
      " 182  Phono_N_H_lemma_aw                        4 non-null      float64\n",
      " 183  OG_N_lemma_aw                             4 non-null      float64\n",
      " 184  OG_N_H_lemma_aw                           4 non-null      float64\n",
      " 185  Freq_N_lemma_aw                           4 non-null      float64\n",
      " 186  Freq_N_P_lemma_aw                         4 non-null      float64\n",
      " 187  Freq_N_PH_lemma_aw                        4 non-null      float64\n",
      " 188  Freq_N_OG_lemma_aw                        4 non-null      float64\n",
      " 189  Freq_N_OGH_lemma_aw                       4 non-null      float64\n",
      " 190  OLD_lemma_aw                              4 non-null      float64\n",
      " 191  OLDF_lemma_aw                             4 non-null      float64\n",
      " 192  PLD_lemma_aw                              4 non-null      float64\n",
      " 193  PLDF_lemma_aw                             4 non-null      float64\n",
      " 194  subtlexus_log_freq_lemma_aw               4 non-null      float64\n",
      " 195  subtlexus_log_cd_lemma_aw                 4 non-null      float64\n",
      " 196  coca_maga_cd_lemma_aw                     4 non-null      float64\n",
      " 197  coca_mag_log_freq_lemma_aw                4 non-null      float64\n",
      " 198  num_rhymes_full_elp_lemma_aw              4 non-null      float64\n",
      " 199  num_rhymes_1000_coca_lemma_aw             4 non-null      float64\n",
      " 200  num_rhymes_2500_coca_lemma_aw             4 non-null      float64\n",
      " 201  num_rhymes_5000_coca_lemma_aw             4 non-null      float64\n",
      " 202  num_rhymes_10000_coca_lemma_aw            4 non-null      float64\n",
      "dtypes: float64(187), int64(9), object(7)\n",
      "memory usage: 6.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df2_info = df2.info(max_cols=len(df2.columns))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6617aff-0fb0-472b-97cc-a41cd17a4ee9",
   "metadata": {},
   "source": [
    "**THIS IS THE MAIN CODE FOR LEMMATIZED DATA**\n",
    "\n",
    "        - Content words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fce52568-a126-4476-984d-08fa33ee1059",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_syllables_lemma_cw_text = []\n",
    "num_letters_lemma_cw_text = []\n",
    "num_phonemes_lemma_cw_text = []\n",
    "discrepancy_raw_lemma_cw_text = []\n",
    "discrepancy_ratio_lemma_cw_text = []\n",
    "avg_syllable_length_lemma_cw_text = []\n",
    "num_consonants_characters_lemma_cw_text = []\n",
    "num_vowel_characters_lemma_cw_text = []\n",
    "num_consonants_phonemes_lemma_cw_text = []\n",
    "num_vowel_phonemes_lemma_cw_text = []\n",
    "avg_phonemes_per_character_consonants_lemma_cw_text = []\n",
    "avg_phonemes_per_character_vowels_lemma_cw_text = []\n",
    "avg_phonemes_per_character_all_lemma_cw_text = []\n",
    "prior_prob_cons_lemma_cw_text = []\n",
    "max_prob_cons_lemma_cw_text = []\n",
    "min_prob_cons_lemma_cw_text = []\n",
    "mid_prob_cons_lemma_cw_text = []\n",
    "number_phonemes_cons_lemma_cw_text = []\n",
    "prior_prob_vowel_lemma_cw_text = []\n",
    "max_prob_vowel_lemma_cw_text = []\n",
    "min_prob_vowel_lemma_cw_text = []\n",
    "mid_prob_vowel_lemma_cw_text = []\n",
    "number_phonemes_vowel_lemma_cw_text = []\n",
    "prior_prob_all_lemma_cw_text = []\n",
    "max_prob_all_lemma_cw_text = []\n",
    "mid_prob_all_lemma_cw_text = []\n",
    "min_prob_all_lemma_cw_text = []\n",
    "number_phonemes_all_lemma_cw_text = []\n",
    "Conditional_Probability_Average_lemma_cw_text = []\n",
    "Ortho_N_lemma_cw_text = []\n",
    "Phono_N_lemma_cw_text = []\n",
    "Phono_N_H_lemma_cw_text = []\n",
    "OG_N_lemma_cw_text = []\n",
    "OG_N_H_lemma_cw_text = []\n",
    "Freq_N_lemma_cw_text = []\n",
    "Freq_N_P_lemma_cw_text = []\n",
    "Freq_N_PH_lemma_cw_text = []\n",
    "Freq_N_OG_lemma_cw_text = []\n",
    "Freq_N_OGH_lemma_cw_text = []\n",
    "OLD_lemma_cw_text = []\n",
    "OLDF_lemma_cw_text = []\n",
    "PLD_lemma_cw_text = []\n",
    "PLDF_lemma_cw_text = []\n",
    "subtlexus_log_freq_lemma_cw_text = []\n",
    "subtlexus_log_cd_lemma_cw_text = []\n",
    "coca_maga_cd_lemma_cw_text = []\n",
    "coca_mag_log_freq_lemma_cw_text = []\n",
    "num_rhymes_full_elp_lemma_cw_text = []\n",
    "num_rhymes_1000_coca_lemma_cw_text = []\n",
    "num_rhymes_2500_coca_lemma_cw_text = []\n",
    "num_rhymes_5000_coca_lemma_cw_text = []\n",
    "num_rhymes_10000_coca_lemma_cw_text = []\n",
    "\n",
    "for tokenized_doc in df_docs:\n",
    "    num_syllables_lemma_cw = []\n",
    "    num_letters_lemma_cw = []\n",
    "    num_phonemes_lemma_cw = []\n",
    "    discrepancy_raw_lemma_cw = []\n",
    "    discrepancy_ratio_lemma_cw = []\n",
    "    avg_syllable_length_lemma_cw = []\n",
    "    num_consonants_characters_lemma_cw = []\n",
    "    num_vowel_characters_lemma_cw = []\n",
    "    num_consonants_phonemes_lemma_cw = []\n",
    "    num_vowel_phonemes_lemma_cw = []\n",
    "    avg_phonemes_per_character_consonants_lemma_cw = []\n",
    "    avg_phonemes_per_character_vowels_lemma_cw = []\n",
    "    avg_phonemes_per_character_all_lemma_cw = []\n",
    "    prior_prob_cons_lemma_cw = []\n",
    "    max_prob_cons_lemma_cw = []\n",
    "    min_prob_cons_lemma_cw = []\n",
    "    mid_prob_cons_lemma_cw = []\n",
    "    number_phonemes_cons_lemma_cw = []\n",
    "    prior_prob_vowel_lemma_cw = []\n",
    "    max_prob_vowel_lemma_cw = []\n",
    "    min_prob_vowel_lemma_cw = []\n",
    "    mid_prob_vowel_lemma_cw = []\n",
    "    number_phonemes_vowel_lemma_cw = []\n",
    "    prior_prob_all_lemma_cw = []\n",
    "    max_prob_all_lemma_cw = []\n",
    "    mid_prob_all_lemma_cw = []\n",
    "    min_prob_all_lemma_cw = []\n",
    "    number_phonemes_all_lemma_cw = []\n",
    "    Conditional_Probability_Average_lemma_cw = []\n",
    "    Ortho_N_lemma_cw = []\n",
    "    Phono_N_lemma_cw = []\n",
    "    Phono_N_H_lemma_cw = []\n",
    "    OG_N_lemma_cw = []\n",
    "    OG_N_H_lemma_cw = []\n",
    "    Freq_N_lemma_cw = []\n",
    "    Freq_N_P_lemma_cw = []\n",
    "    Freq_N_PH_lemma_cw = []\n",
    "    Freq_N_OG_lemma_cw = []\n",
    "    Freq_N_OGH_lemma_cw = []\n",
    "    OLD_lemma_cw = []\n",
    "    OLDF_lemma_cw = []\n",
    "    PLD_lemma_cw = []\n",
    "    PLDF_lemma_cw = []\n",
    "    subtlexus_log_freq_lemma_cw = []\n",
    "    subtlexus_log_cd_lemma_cw = []\n",
    "    coca_maga_cd_lemma_cw = []\n",
    "    coca_mag_log_freq_lemma_cw = []\n",
    "    num_rhymes_full_elp_lemma_cw = []\n",
    "    num_rhymes_1000_coca_lemma_cw = []\n",
    "    num_rhymes_2500_coca_lemma_cw = []\n",
    "    num_rhymes_5000_coca_lemma_cw = []\n",
    "    num_rhymes_10000_coca_lemma_cw = []\n",
    "    for token in tokenized_doc:\n",
    "        if not token.is_stop and not token.is_punct:\n",
    "            #print(token.text, token.lemma_)\n",
    "            try:\n",
    "                val = decoding_dic[token.lemma_]\n",
    "                #print(key)\n",
    "                #print(val[0])\n",
    "                num_syllables_lemma_cw.append(val[0])\n",
    "                num_letters_lemma_cw.append(val[1])\n",
    "                num_phonemes_lemma_cw.append(val[2])\n",
    "                discrepancy_raw_lemma_cw.append(val[3])\n",
    "                discrepancy_ratio_lemma_cw.append(val[4])\n",
    "                avg_syllable_length_lemma_cw.append(val[5])\n",
    "                num_consonants_characters_lemma_cw.append(val[6])\n",
    "                num_vowel_characters_lemma_cw.append(val[7])\n",
    "                num_consonants_phonemes_lemma_cw.append(val[8])\n",
    "                num_vowel_phonemes_lemma_cw.append(val[9])\n",
    "                avg_phonemes_per_character_consonants_lemma_cw.append(val[10])\n",
    "                avg_phonemes_per_character_vowels_lemma_cw.append(val[11])\n",
    "                avg_phonemes_per_character_all_lemma_cw.append(val[12])\n",
    "                prior_prob_cons_lemma_cw.append(val[13])\n",
    "                max_prob_cons_lemma_cw.append(val[14])\n",
    "                min_prob_cons_lemma_cw.append(val[15])\n",
    "                mid_prob_cons_lemma_cw.append(val[16])\n",
    "                number_phonemes_cons_lemma_cw.append(val[17])\n",
    "                prior_prob_vowel_lemma_cw.append(val[18])\n",
    "                max_prob_vowel_lemma_cw.append(val[19])\n",
    "                min_prob_vowel_lemma_cw.append(val[20])\n",
    "                mid_prob_vowel_lemma_cw.append(val[21])\n",
    "                number_phonemes_vowel_lemma_cw.append(val[22])\n",
    "                prior_prob_all_lemma_cw.append(val[23])\n",
    "                max_prob_all_lemma_cw.append(val[24])\n",
    "                mid_prob_all_lemma_cw.append(val[25])\n",
    "                min_prob_all_lemma_cw.append(val[26])\n",
    "                number_phonemes_all_lemma_cw.append(val[27])\n",
    "                Conditional_Probability_Average_lemma_cw.append(val[28])\n",
    "                Ortho_N_lemma_cw.append(val[29])\n",
    "                Phono_N_lemma_cw.append(val[30])\n",
    "                Phono_N_H_lemma_cw.append(val[31])\n",
    "                OG_N_lemma_cw.append(val[32])\n",
    "                OG_N_H_lemma_cw.append(val[33])\n",
    "                Freq_N_lemma_cw.append(val[34])\n",
    "                Freq_N_P_lemma_cw.append(val[35])\n",
    "                Freq_N_PH_lemma_cw.append(val[36])\n",
    "                Freq_N_OG_lemma_cw.append(val[37])\n",
    "                Freq_N_OGH_lemma_cw.append(val[38])\n",
    "                OLD_lemma_cw.append(val[39])\n",
    "                OLDF_lemma_cw.append(val[40])\n",
    "                PLD_lemma_cw.append(val[41])\n",
    "                PLDF_lemma_cw.append(val[42])\n",
    "                subtlexus_log_freq_lemma_cw.append(val[43])\n",
    "                subtlexus_log_cd_lemma_cw.append(val[44])\n",
    "                coca_maga_cd_lemma_cw.append(val[45])\n",
    "                coca_mag_log_freq_lemma_cw.append(val[46])\n",
    "                num_rhymes_full_elp_lemma_cw.append(val[47])\n",
    "                num_rhymes_1000_coca_lemma_cw.append(val[48])\n",
    "                num_rhymes_2500_coca_lemma_cw.append(val[49])\n",
    "                num_rhymes_5000_coca_lemma_cw.append(val[50])\n",
    "                num_rhymes_10000_coca_lemma_cw.append(val[51])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    num_syllables_lemma_cw_text.append(num_syllables_lemma_cw)\n",
    "    num_letters_lemma_cw_text.append(num_letters_lemma_cw)\n",
    "    num_phonemes_lemma_cw_text.append(num_phonemes_lemma_cw)\n",
    "    discrepancy_raw_lemma_cw_text.append(discrepancy_raw_lemma_cw)\n",
    "    discrepancy_ratio_lemma_cw_text.append(discrepancy_ratio_lemma_cw)\n",
    "    avg_syllable_length_lemma_cw_text.append(avg_syllable_length_lemma_cw)\n",
    "    num_consonants_characters_lemma_cw_text.append(num_consonants_characters_lemma_cw)\n",
    "    num_vowel_characters_lemma_cw_text.append(num_vowel_characters_lemma_cw)\n",
    "    num_consonants_phonemes_lemma_cw_text.append(num_consonants_phonemes_lemma_cw)\n",
    "    num_vowel_phonemes_lemma_cw_text.append(num_vowel_phonemes_lemma_cw)\n",
    "    avg_phonemes_per_character_consonants_lemma_cw_text.append(avg_phonemes_per_character_consonants_lemma_cw)\n",
    "    avg_phonemes_per_character_vowels_lemma_cw_text.append(avg_phonemes_per_character_vowels_lemma_cw)\n",
    "    avg_phonemes_per_character_all_lemma_cw_text.append(avg_phonemes_per_character_all_lemma_cw)\n",
    "    prior_prob_cons_lemma_cw_text.append(prior_prob_cons_lemma_cw)\n",
    "    max_prob_cons_lemma_cw_text.append(max_prob_cons_lemma_cw)\n",
    "    min_prob_cons_lemma_cw_text.append(min_prob_cons_lemma_cw)\n",
    "    mid_prob_cons_lemma_cw_text.append(mid_prob_cons_lemma_cw)\n",
    "    number_phonemes_cons_lemma_cw_text.append(number_phonemes_cons_lemma_cw)\n",
    "    prior_prob_vowel_lemma_cw_text.append(prior_prob_vowel_lemma_cw)\n",
    "    max_prob_vowel_lemma_cw_text.append(max_prob_vowel_lemma_cw)\n",
    "    min_prob_vowel_lemma_cw_text.append(min_prob_vowel_lemma_cw)\n",
    "    mid_prob_vowel_lemma_cw_text.append(mid_prob_vowel_lemma_cw)\n",
    "    number_phonemes_vowel_lemma_cw_text.append(number_phonemes_vowel_lemma_cw)\n",
    "    prior_prob_all_lemma_cw_text.append(prior_prob_all_lemma_cw)\n",
    "    max_prob_all_lemma_cw_text.append(max_prob_all_lemma_cw)\n",
    "    mid_prob_all_lemma_cw_text.append(mid_prob_all_lemma_cw)\n",
    "    min_prob_all_lemma_cw_text.append(min_prob_all_lemma_cw)\n",
    "    number_phonemes_all_lemma_cw_text.append(number_phonemes_all_lemma_cw)\n",
    "    Conditional_Probability_Average_lemma_cw_text.append(Conditional_Probability_Average_lemma_cw)\n",
    "    Ortho_N_lemma_cw_text.append(Ortho_N_lemma_cw)\n",
    "    Phono_N_lemma_cw_text.append(Phono_N_lemma_cw)\n",
    "    Phono_N_H_lemma_cw_text.append(Phono_N_H_lemma_cw)\n",
    "    OG_N_lemma_cw_text.append(OG_N_lemma_cw)\n",
    "    OG_N_H_lemma_cw_text.append(OG_N_H_lemma_cw)\n",
    "    Freq_N_lemma_cw_text.append(Freq_N_lemma_cw)\n",
    "    Freq_N_P_lemma_cw_text.append(Freq_N_P_lemma_cw)\n",
    "    Freq_N_PH_lemma_cw_text.append(Freq_N_PH_lemma_cw)\n",
    "    Freq_N_OG_lemma_cw_text.append(Freq_N_OG_lemma_cw)\n",
    "    Freq_N_OGH_lemma_cw_text.append(Freq_N_OGH_lemma_cw)\n",
    "    OLD_lemma_cw_text.append(OLD_lemma_cw)\n",
    "    OLDF_lemma_cw_text.append(OLDF_lemma_cw)\n",
    "    PLD_lemma_cw_text.append(PLD_lemma_cw)\n",
    "    PLDF_lemma_cw_text.append(PLDF_lemma_cw)\n",
    "    subtlexus_log_freq_lemma_cw_text.append(subtlexus_log_freq_lemma_cw)\n",
    "    subtlexus_log_cd_lemma_cw_text.append(subtlexus_log_cd_lemma_cw)\n",
    "    coca_maga_cd_lemma_cw_text.append(coca_maga_cd_lemma_cw)\n",
    "    coca_mag_log_freq_lemma_cw_text.append(coca_mag_log_freq_lemma_cw)\n",
    "    num_rhymes_full_elp_lemma_cw_text.append(num_rhymes_full_elp_lemma_cw)\n",
    "    num_rhymes_1000_coca_lemma_cw_text.append(num_rhymes_1000_coca_lemma_cw)\n",
    "    num_rhymes_2500_coca_lemma_cw_text.append(num_rhymes_2500_coca_lemma_cw)\n",
    "    num_rhymes_5000_coca_lemma_cw_text.append(num_rhymes_5000_coca_lemma_cw)\n",
    "    num_rhymes_10000_coca_lemma_cw_text.append(num_rhymes_10000_coca_lemma_cw)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e421b74c-7ec9-43c7-81fb-cb41dcd5ae29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0            this is a simple practice text with ghlq.\n",
      "1                                   same, but simpler.\n",
      "2    words increasing, i think. about aaberg and aa...\n",
      "3                                          it will be.\n",
      "Name: Excerpt, dtype: object\n",
      "this is a list of number of syllables for each lemma in the text [[2.0, 2.0, 1.0], [2.0], [1.0, 2.0, 1.0, 2.0, 3.0, 3.0], []]\n",
      "this is a list of number of syllables for each word in the text [[2.0, 2.0, 1.0], [3.0], [1.0, 3.0, 1.0, 2.0, 3.0, 3.0], []]\n",
      "this is a list of number of letters for each lemma in the text [[6.0, 8.0, 4.0], [6.0], [4.0, 8.0, 5.0, 6.0, 3.0, 7.0], []]\n",
      "this is a list of number of letters for each word in the text [[6.0, 8.0, 4.0], [7.0], [5.0, 10.0, 5.0, 6.0, 3.0, 7.0], []]\n"
     ]
    }
   ],
   "source": [
    "print(df['Excerpt'])\n",
    "\n",
    "#sanity checks\n",
    "print(f'this is a list of number of syllables for each lemma in the text {num_syllables_lemma_cw_text}')\n",
    "print(f'this is a list of number of syllables for each word in the text {num_syllables_token_cw_text}')\n",
    "\n",
    "print(f'this is a list of number of letters for each lemma in the text {num_letters_lemma_cw_text}')\n",
    "print(f'this is a list of number of letters for each word in the text {num_letters_token_cw_text}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d363cd47-69b1-4c48-8633-fddd791bb48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove nan's from the list of lists\n",
    "\n",
    "Conditional_Probability_Average_lemma_cw_text_no_nan = remove_nan(Conditional_Probability_Average_lemma_cw_text)\n",
    "Ortho_N_lemma_cw_text_no_nan = remove_nan(Ortho_N_lemma_cw_text)\n",
    "Phono_N_lemma_cw_text_no_nan = remove_nan(Phono_N_lemma_cw_text)\n",
    "Phono_N_H_lemma_cw_text_no_nan = remove_nan(Phono_N_H_lemma_cw_text)\n",
    "OG_N_lemma_cw_text_no_nan = remove_nan(OG_N_lemma_cw_text)\n",
    "OG_N_H_lemma_cw_text_no_nan = remove_nan(OG_N_H_lemma_cw_text)\n",
    "Freq_N_lemma_cw_text_no_nan = remove_nan(Freq_N_lemma_cw_text)\n",
    "Freq_N_P_lemma_cw_text_no_nan = remove_nan(Freq_N_P_lemma_cw_text)\n",
    "Freq_N_PH_lemma_cw_text_no_nan = remove_nan(Freq_N_PH_lemma_cw_text)\n",
    "Freq_N_OG_lemma_cw_text_no_nan = remove_nan(Freq_N_OG_lemma_cw_text)\n",
    "Freq_N_OGH_lemma_cw_text_no_nan = remove_nan(Freq_N_OGH_lemma_cw_text)\n",
    "OLD_lemma_cw_text_no_nan = remove_nan(OLD_lemma_cw_text)\n",
    "OLDF_lemma_cw_text_no_nan = remove_nan(OLDF_lemma_cw_text)\n",
    "PLD_lemma_cw_text_no_nan = remove_nan(PLD_lemma_cw_text)\n",
    "PLDF_lemma_cw_text_no_nan = remove_nan(PLDF_lemma_cw_text)\n",
    "subtlexus_log_freq_lemma_cw_text_no_nan = remove_nan(subtlexus_log_freq_lemma_cw_text)\n",
    "subtlexus_log_cd_lemma_cw_text_no_nan = remove_nan(subtlexus_log_cd_lemma_cw_text)\n",
    "coca_maga_cd_lemma_cw_text_no_nan = remove_nan(coca_maga_cd_lemma_cw_text)\n",
    "coca_mag_log_freq_lemma_cw_text_no_nan = remove_nan(coca_mag_log_freq_lemma_cw_text)\n",
    "num_rhymes_full_elp_lemma_cw_text_no_nan = remove_nan(num_rhymes_full_elp_lemma_cw_text)\n",
    "num_rhymes_1000_coca_lemma_cw_text_no_nan = remove_nan(num_rhymes_1000_coca_lemma_cw_text)\n",
    "num_rhymes_2500_coca_lemma_cw_text_no_nan = remove_nan(num_rhymes_2500_coca_lemma_cw_text)\n",
    "num_rhymes_5000_coca_lemma_cw_text_no_nan = remove_nan(num_rhymes_5000_coca_lemma_cw_text)\n",
    "num_rhymes_10000_coca_lemma_cw_text_no_nan = remove_nan(num_rhymes_10000_coca_lemma_cw_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d34bb4e6-3115-48d0-8ed0-6b5b9014ed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#get lists that are average of sublists\n",
    "\n",
    "num_syl_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_syllables_lemma_cw_text]#if it is a sublist, get average, else (if it is not a sublist, empty list, return 0) \n",
    "num_let_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_letters_lemma_cw_text]\n",
    "num_phone_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_phonemes_lemma_cw_text]\n",
    "discrepancy_raw_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in discrepancy_raw_lemma_cw_text]\n",
    "discrepancy_ratio_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in discrepancy_ratio_lemma_cw_text]\n",
    "avg_syl_length_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in avg_syllable_length_lemma_cw_text]\n",
    "num_cons_char_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_consonants_characters_lemma_cw_text]\n",
    "num_vowel_char_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_vowel_characters_lemma_cw_text]\n",
    "num_cons_phone_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_consonants_phonemes_lemma_cw_text]\n",
    "num_vowel_phone_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_vowel_phonemes_lemma_cw_text]\n",
    "avg_phone_per_char_cons_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in avg_phonemes_per_character_consonants_lemma_cw_text]\n",
    "avg_phone_per_char_vowel_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in avg_phonemes_per_character_vowels_lemma_cw_text]\n",
    "avg_phone_per_char_all_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in avg_phonemes_per_character_all_lemma_cw_text]\n",
    "prior_prob_cons_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in prior_prob_cons_lemma_cw_text]\n",
    "max_prob_cons_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in max_prob_cons_lemma_cw_text]\n",
    "min_prob_cons_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in min_prob_cons_lemma_cw_text]\n",
    "mid_prob_cons_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in mid_prob_cons_lemma_cw_text]\n",
    "number_phone_cons_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in number_phonemes_cons_lemma_cw_text]\n",
    "prior_prob_vowel_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in prior_prob_vowel_lemma_cw_text]\n",
    "max_prob_vowel_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in max_prob_vowel_lemma_cw_text]\n",
    "min_prob_vowel_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in min_prob_vowel_lemma_cw_text]\n",
    "mid_prob_vowel_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in mid_prob_vowel_lemma_cw_text]\n",
    "number_phone_vowel_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in number_phonemes_vowel_lemma_cw_text]\n",
    "prior_prob_all_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in prior_prob_all_lemma_cw_text]\n",
    "max_prob_all_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in max_prob_all_lemma_cw_text]\n",
    "mid_prob_all_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in mid_prob_all_lemma_cw_text]\n",
    "min_prob_all_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in min_prob_all_lemma_cw_text]\n",
    "number_phone_all_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in number_phonemes_all_lemma_cw_text]\n",
    "Conditional_Probability_Average_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in Conditional_Probability_Average_lemma_cw_text_no_nan]\n",
    "Ortho_N_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in Ortho_N_lemma_cw_text_no_nan]\n",
    "Phono_N_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in Phono_N_lemma_cw_text_no_nan]\n",
    "Phono_N_H_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in Phono_N_H_lemma_cw_text_no_nan]\n",
    "OG_N_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in OG_N_lemma_cw_text_no_nan]\n",
    "OG_N_H_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in OG_N_H_lemma_cw_text_no_nan]\n",
    "Freq_N_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in Freq_N_lemma_cw_text_no_nan]\n",
    "Freq_N_P_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in Freq_N_P_lemma_cw_text_no_nan]\n",
    "Freq_N_PH_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in Freq_N_PH_lemma_cw_text_no_nan]\n",
    "Freq_N_OG_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in Freq_N_OG_lemma_cw_text_no_nan]\n",
    "Freq_N_OGH_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in Freq_N_OGH_lemma_cw_text_no_nan]\n",
    "OLD_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in OLD_lemma_cw_text_no_nan]\n",
    "OLDF_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in OLDF_lemma_cw_text_no_nan]\n",
    "PLD_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in PLD_lemma_cw_text_no_nan]\n",
    "PLDF_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in PLDF_lemma_cw_text_no_nan]\n",
    "subtlexus_log_freq_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in subtlexus_log_freq_lemma_cw_text_no_nan]\n",
    "subtlexus_log_cd_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in subtlexus_log_cd_lemma_cw_text_no_nan]\n",
    "coca_maga_cd_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in coca_maga_cd_lemma_cw_text_no_nan]\n",
    "coca_mag_log_freq_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in coca_mag_log_freq_lemma_cw_text_no_nan]\n",
    "num_rhymes_full_elp_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_rhymes_full_elp_lemma_cw_text_no_nan]\n",
    "num_rhymes_1000_coca_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_rhymes_1000_coca_lemma_cw_text_no_nan]\n",
    "num_rhymes_2500_coca_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_rhymes_2500_coca_lemma_cw_text_no_nan]\n",
    "num_rhymes_5000_coca_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_rhymes_5000_coca_lemma_cw_text_no_nan]\n",
    "num_rhymes_10000_coca_lemma_cw = [sum(sub_list) / len(sub_list) if sub_list else 0 for sub_list in num_rhymes_10000_coca_lemma_cw_text_no_nan]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "49809f61-8cd8-4f9b-865e-92d719183863",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add to database\n",
    "\n",
    "df2 = df2.assign(num_syl_lemma_cw = num_syl_lemma_cw, \n",
    "                num_let_lemma_cw = num_let_lemma_cw, \n",
    "                num_phone_lemma_cw = num_phone_lemma_cw, \n",
    "                discrepancy_raw_lemma_cw = discrepancy_raw_lemma_cw, \n",
    "                discrepancy_ratio_lemma_cw = discrepancy_ratio_lemma_cw, \n",
    "                avg_syl_length_lemma_cw = avg_syl_length_lemma_cw, \n",
    "                num_cons_char_lemma_cw = num_cons_char_lemma_cw, \n",
    "                num_vowel_char_lemma_cw = num_vowel_char_lemma_cw, \n",
    "                num_cons_phone_lemma_cw = num_cons_phone_lemma_cw, \n",
    "                num_vowel_phone_lemma_cw = num_vowel_phone_lemma_cw, \n",
    "                avg_phone_per_char_cons_lemma_cw = avg_phone_per_char_cons_lemma_cw, \n",
    "                avg_phone_per_char_vowel_lemma_cw = avg_phone_per_char_vowel_lemma_cw, \n",
    "                avg_phone_per_char_all_lemma_cw = avg_phone_per_char_all_lemma_cw, \n",
    "                reverse_prior_prob_cons_lemma_cw = prior_prob_cons_lemma_cw, \n",
    "                max_prob_cons_lemma_cw = max_prob_cons_lemma_cw, \n",
    "                min_prob_cons_lemma_cw = min_prob_cons_lemma_cw, \n",
    "                mid_prob_cons_lemma_cw = mid_prob_cons_lemma_cw, \n",
    "                number_phone_cons_lemma_cw = number_phone_cons_lemma_cw, \n",
    "                reverse_prior_prob_vowel_lemma_cw = prior_prob_vowel_lemma_cw, \n",
    "                max_prob_vowel_lemma_cw = max_prob_vowel_lemma_cw, \n",
    "                min_prob_vowel_lemma_cw = min_prob_vowel_lemma_cw, \n",
    "                mid_prob_vowel_lemma_cw = mid_prob_vowel_lemma_cw, \n",
    "                number_phone_vowel_lemma_cw = number_phone_vowel_lemma_cw, \n",
    "                reverse_prior_prob_all_lemma_cw = prior_prob_all_lemma_cw, \n",
    "                max_prob_all_lemma_cw = max_prob_all_lemma_cw, \n",
    "                mid_prob_all_lemma_cw = mid_prob_all_lemma_cw, \n",
    "                min_prob_all_lemma_cw = min_prob_all_lemma_cw, \n",
    "                number_phone_all_lemma_cw = number_phone_all_lemma_cw,\n",
    "                Conditional_Probability_Average_lemma_cw = Conditional_Probability_Average_lemma_cw,\n",
    "                Ortho_N_lemma_cw = Ortho_N_lemma_cw,\n",
    "                Phono_N_lemma_cw = Phono_N_lemma_cw,\n",
    "                Phono_N_H_lemma_cw = Phono_N_H_lemma_cw,\n",
    "                OG_N_lemma_cw = OG_N_lemma_cw,\n",
    "                OG_N_H_lemma_cw = OG_N_H_lemma_cw,\n",
    "                Freq_N_lemma_cw = Freq_N_lemma_cw,\n",
    "                Freq_N_P_lemma_cw = Freq_N_P_lemma_cw,\n",
    "                Freq_N_PH_lemma_cw = Freq_N_PH_lemma_cw,\n",
    "                Freq_N_OG_lemma_cw = Freq_N_OG_lemma_cw,\n",
    "                Freq_N_OGH_lemma_cw = Freq_N_OGH_lemma_cw,\n",
    "                OLD_lemma_cw = OLD_lemma_cw,\n",
    "                OLDF_lemma_cw = OLDF_lemma_cw,\n",
    "                PLD_lemma_cw = PLD_lemma_cw,\n",
    "                PLDF_lemma_cw = PLDF_lemma_cw,\n",
    "                subtlexus_log_freq_lemma_cw = subtlexus_log_freq_lemma_cw,\n",
    "                subtlexus_log_cd_lemma_cw = subtlexus_log_cd_lemma_cw,\n",
    "                coca_maga_cd_lemma_cw = coca_maga_cd_lemma_cw,\n",
    "                coca_mag_log_freq_lemma_cw = coca_mag_log_freq_lemma_cw,\n",
    "                num_rhymes_full_elp_lemma_cw = num_rhymes_full_elp_lemma_cw,\n",
    "                num_rhymes_1000_coca_lemma_cw = num_rhymes_1000_coca_lemma_cw,\n",
    "                num_rhymes_2500_coca_lemma_cw = num_rhymes_2500_coca_lemma_cw,\n",
    "                num_rhymes_5000_coca_lemma_cw = num_rhymes_5000_coca_lemma_cw,\n",
    "                num_rhymes_10000_coca_lemma_cw = num_rhymes_10000_coca_lemma_cw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d466d515-1c6d-406a-9f3a-b3d36b846edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 255)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0e5877ce-753f-496f-b34e-85e635a615d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Author</th>\n",
       "      <th>Title</th>\n",
       "      <th>Anthology</th>\n",
       "      <th>URL</th>\n",
       "      <th>Pub Year</th>\n",
       "      <th>Categ</th>\n",
       "      <th>Sub Cat</th>\n",
       "      <th>Lexile Band</th>\n",
       "      <th>Location</th>\n",
       "      <th>...</th>\n",
       "      <th>PLDF_lemma_cw</th>\n",
       "      <th>subtlexus_log_freq_lemma_cw</th>\n",
       "      <th>subtlexus_log_cd_lemma_cw</th>\n",
       "      <th>coca_maga_cd_lemma_cw</th>\n",
       "      <th>coca_mag_log_freq_lemma_cw</th>\n",
       "      <th>num_rhymes_full_elp_lemma_cw</th>\n",
       "      <th>num_rhymes_1000_coca_lemma_cw</th>\n",
       "      <th>num_rhymes_2500_coca_lemma_cw</th>\n",
       "      <th>num_rhymes_5000_coca_lemma_cw</th>\n",
       "      <th>num_rhymes_10000_coca_lemma_cw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>400</td>\n",
       "      <td>Carolyn Wells</td>\n",
       "      <td>Patty's Suitors</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.gutenberg.org/cache/epub/5631/pg563...</td>\n",
       "      <td>1914</td>\n",
       "      <td>Lit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>900</td>\n",
       "      <td>mid</td>\n",
       "      <td>...</td>\n",
       "      <td>7.446667</td>\n",
       "      <td>3.15140</td>\n",
       "      <td>2.973733</td>\n",
       "      <td>0.126715</td>\n",
       "      <td>3.983744</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>401</td>\n",
       "      <td>Carolyn Wells</td>\n",
       "      <td>Two Little Women on a Holiday</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.gutenberg.org/cache/epub/5893/pg589...</td>\n",
       "      <td>1917</td>\n",
       "      <td>Lit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>700</td>\n",
       "      <td>mid</td>\n",
       "      <td>...</td>\n",
       "      <td>7.210000</td>\n",
       "      <td>3.65860</td>\n",
       "      <td>3.456700</td>\n",
       "      <td>0.212978</td>\n",
       "      <td>4.252195</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>402</td>\n",
       "      <td>Carolyn Wells</td>\n",
       "      <td>Patty Blossom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.gutenberg.org/cache/epub/20945/pg20...</td>\n",
       "      <td>1917</td>\n",
       "      <td>Lit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>900</td>\n",
       "      <td>mid</td>\n",
       "      <td>...</td>\n",
       "      <td>7.647500</td>\n",
       "      <td>3.23982</td>\n",
       "      <td>2.875200</td>\n",
       "      <td>0.148607</td>\n",
       "      <td>3.724659</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>9.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>403</td>\n",
       "      <td>CHARLES KINGSLEY</td>\n",
       "      <td>THE WATER-BABIES\\nA Fairy Tale for a Land-Baby</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.gutenberg.org/files/25564/25564-h/2...</td>\n",
       "      <td>1863</td>\n",
       "      <td>Lit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1300</td>\n",
       "      <td>mid</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 255 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID            Author                                           Title  \\\n",
       "0  400     Carolyn Wells                                 Patty's Suitors   \n",
       "1  401     Carolyn Wells                   Two Little Women on a Holiday   \n",
       "2  402     Carolyn Wells                                   Patty Blossom   \n",
       "3  403  CHARLES KINGSLEY  THE WATER-BABIES\\nA Fairy Tale for a Land-Baby   \n",
       "\n",
       "   Anthology                                                URL  Pub Year  \\\n",
       "0        NaN  http://www.gutenberg.org/cache/epub/5631/pg563...      1914   \n",
       "1        NaN  http://www.gutenberg.org/cache/epub/5893/pg589...      1917   \n",
       "2        NaN  http://www.gutenberg.org/cache/epub/20945/pg20...      1917   \n",
       "3        NaN  http://www.gutenberg.org/files/25564/25564-h/2...      1863   \n",
       "\n",
       "  Categ  Sub Cat  Lexile Band Location  ...  PLDF_lemma_cw  \\\n",
       "0   Lit      NaN          900      mid  ...       7.446667   \n",
       "1   Lit      NaN          700      mid  ...       7.210000   \n",
       "2   Lit      NaN          900      mid  ...       7.647500   \n",
       "3   Lit      NaN         1300      mid  ...       0.000000   \n",
       "\n",
       "  subtlexus_log_freq_lemma_cw  subtlexus_log_cd_lemma_cw  \\\n",
       "0                     3.15140                   2.973733   \n",
       "1                     3.65860                   3.456700   \n",
       "2                     3.23982                   2.875200   \n",
       "3                     0.00000                   0.000000   \n",
       "\n",
       "   coca_maga_cd_lemma_cw coca_mag_log_freq_lemma_cw  \\\n",
       "0               0.126715                   3.983744   \n",
       "1               0.212978                   4.252195   \n",
       "2               0.148607                   3.724659   \n",
       "3               0.000000                   0.000000   \n",
       "\n",
       "   num_rhymes_full_elp_lemma_cw  num_rhymes_1000_coca_lemma_cw  \\\n",
       "0                      2.666667                            0.0   \n",
       "1                      2.000000                            0.0   \n",
       "2                     21.000000                            1.5   \n",
       "3                      0.000000                            0.0   \n",
       "\n",
       "   num_rhymes_2500_coca_lemma_cw  num_rhymes_5000_coca_lemma_cw  \\\n",
       "0                       0.333333                       0.333333   \n",
       "1                       0.000000                       0.000000   \n",
       "2                       2.500000                       4.250000   \n",
       "3                       0.000000                       0.000000   \n",
       "\n",
       "   num_rhymes_10000_coca_lemma_cw  \n",
       "0                        0.333333  \n",
       "1                        0.000000  \n",
       "2                        9.750000  \n",
       "3                        0.000000  \n",
       "\n",
       "[4 rows x 255 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a399ac8a-b821-4e59-b230-ca3657cfed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv(\"decoding_features_clear.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
